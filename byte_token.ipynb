{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01306f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import nltk\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40af0e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61ea9627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda!\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e1b1c6",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b3462636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "path_to_data = \"data/wiki.train.tokens/wiki.train.tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "40007e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a tokenizer from your text files\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(files=[path_to_data], vocab_size=30_000, min_frequency=2, special_tokens=['<SOS>', '<EOS>', '<PAD>', '<UNK>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f924610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "all_data = []\n",
    "with open(path_to_data, 'r', encoding=\"utf-8\") as f:\n",
    "    curr = []\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        \n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        elif line[0] == '=':\n",
    "            all_data.append(' '.join(curr))\n",
    "            curr = []\n",
    "        else:\n",
    "            curr.append(line)\n",
    "\n",
    "wiki_tokens = [tokenizer.encode(line).ids for line in all_data]\n",
    "\n",
    "wiki_tokens.sort(key=lambda x: len(x))\n",
    "train_tokens = [s for s in wiki_tokens if s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d235d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Val\n",
    "path_to_data = \"data/wiki.valid.tokens/wiki.valid.tokens\"\n",
    "\n",
    "all_data = []\n",
    "with open(path_to_data, 'r', encoding=\"utf-8\") as f:\n",
    "    curr = []\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        \n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        elif line[0] == '=':\n",
    "            all_data.append(' '.join(curr))\n",
    "            curr = []\n",
    "        else:\n",
    "            curr.append(line)\n",
    "\n",
    "wiki_tokens = [tokenizer.encode(line).ids for line in all_data]\n",
    "\n",
    "wiki_tokens.sort(key=lambda x: len(x))\n",
    "val_tokens = [s for s in wiki_tokens if s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "39ac35bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2004 – 05 season Before the 2004 – 05 NBA season , the Mavericks were re @-@ tooled again . Defensive center Erick Dampier was acquired from the Golden State Warriors , but Nowitzki \\'s close friend Steve Nash left Dallas and returned to the Phoenix Suns as a free agent . During the season , long @-@ time head coach Don Nelson resigned , and his assistant Avery Johnson took on coaching duties . In the midst of these changes , Nowitzki stepped up his game and averaged 26 @.@ 1 points a game ( a career high ) and 9 @.@ 7 rebounds , and his 1 @.@ 5 blocks and 3 @.@ 1 assists were also career numbers . In addition , Nowitzki scored at least 10 points in every game and was one of four players who registered at least 1 @.@ 2 steals and blocks per game . This was also his second 2 @,@ 000 @-@ point season ; his 26 @.@ 1 points scoring average set a new record by a European player . On December 2 , 2004 , Nowitzki scored 53 points in an overtime win against the Houston Rockets , a career best . As a reward , Nowitzki was voted to the All @-@ NBA First Team for the first time . He also placed third in the league \\'s MVP voting , behind Nash and Shaquille O \\'Neal . By being elected to the All @-@ NBA First Team , Nowitzki became the first player who did not attend a United States high school or college to be on the All @-@ NBA First Team . However , the Mavericks had a subpar 2005 NBA Playoffs campaign . In the first round , Dallas met Houston Rockets scoring champion Tracy McGrady and 7 – 6 center Yao Ming , and Nowitzki was expected to average high figures against unheralded forward Ryan Bowen : nba.com described Bowen as \" overmatched \" versus the German . Instead , Bowen limited Nowitzki to just 21 points in Game 1 and 26 points in Game 2 , where the latter hit 8 of 26 shots from the field . The Rockets took a 2 – 0 series lead before the Mavericks won three games in a row . After losing Game 6 , Dallas won Game 7 convincingly and won the series even though Nowitzki struggled with his shooting . In the Western Conference Semifinals , the Mavericks met the Phoenix Suns , the new club of Nash . They split the first four games , before the Suns won the last two games . In Game 6 , which the Mavericks lost in overtime , Nowitzki was again not at his best : he scored 28 points , but also sank only 9 of his 25 field goal attempts ; in addition , he was visibly irritated , repeatedly shouting at his teammates and missing all five of his shots in OT . 2005 – 06 season Prior to the 2005 – 06 NBA season , veteran Mavericks captain Michael Finley was waived over the summer , and now Nowitzki was the last player remaining from the Mavericks \\' \" Big Three \" of Nash , Finley , and himself . Nowitzki blossomed as the sole franchise player , averaging 26 @.@ 6 points , 9 @.@ 0 rebounds , and 2 @.@ 8 assists . Not only was this his third 2 @,@ 000 @-@ point season , but his scoring average of 26 @.@ 6 points was highest ever by a European . He improved his shooting percentage , setting personal season records in field goals ( 48 @.@ 0 % ) , three @-@ point shots ( 40 @.@ 6 % ) and free throws ( 90 @.@ 1 % ) . During the 2006 All @-@ Star Weekend in Houston , Nowitzki scored 18 points to defeat Seattle SuperSonics guard Ray Allen and Washington Wizards guard Gilbert Arenas in the Three @-@ Point Shootout contest . Nowitzki paced Dallas to a 60 @-@ win season . The team finished with the third @-@ best record in the league , behind the defending champion San Antonio Spurs and defending Eastern Conference champion Detroit Pistons . As in the 2004 – 05 season , he finished third in the league \\'s MVP voting , this time behind Nash and LeBron James . He was again elected to the first team All @-@ NBA squad . Nowitzki confirmed his superstar status during the playoffs as he averaged 27 @.@ 0 points , 11 @.@ 7 rebounds , and 2 @.@ 9 assists . The Mavericks swept the Memphis Grizzlies , 4 – 0 , with Nowitzki scored a clutch three @-@ pointer in the closing seconds of Game 3 which tied the game and forced overtime . In the Western Conference Semifinals , the Mavericks played against the San Antonio Spurs again . After splitting the first six games , the Mavericks took a 20 @-@ point lead in Game 7 before Spur Manu Ginóbili broke a tie at 101 by hitting a 3 with 30 seconds left . On the next play , Nowitzki completed a three @-@ point play , which tied the game at 104 . In the end , the Mavericks won , 119 – 111 , and Nowitzki ended the game with 37 points and 15 rebounds . Nowitzki commented : \" I don \\'t know how the ball went in . Manu hit my hand . It was a lucky bounce . \" The Mavericks advanced to the Western Conference Finals , where they again met the Suns . Nowitzki scored 50 points to lead the Mavericks to a victory in the crucial Game 5 with the series tied at 2 ; the Mavericks won the series in six games and faced the Miami Heat in the 2006 NBA Finals . A content Nowitzki commented : \" We \\'ve been a good road team all season long , we believed in each other . We went through some ups and downs this season , but the playoffs are all about showing heart and playing together . \" Of Nowitzki \\'s performance , ESPN columnist Bill Simmons wrote , \" Dirk is playing at a higher level than any forward since [ Larry ] Bird . \" The Mavericks took an early 2 – 0 lead , but then gave away a late 15 @-@ point lead in a Game 3 loss and finally fell to a scoring onslaught by Heat Finals MVP Dwyane Wade : Wade scored at least 36 points in the next four games , all of which the Heat won . Nowitzki only made 20 of his last 55 shots in the final three games as the Mavericks lost the Finals series , 4 – 2 , to the Heat . The German was criticized by ESPN as \" clearly ... not as his best this series \" and remarked : \" That was a tough loss ( in Game 3 ) and that really changed the whole momentum of the series . ... After that , they got confidence . They played a lot better afterwards . \" 2006 – 07 MVP season The 2006 – 07 season was the year Nowitzki was named the league \\'s Most Valuable Player . He shot a career @-@ best 50 @.@ 2 % from the field , and recorded averages of 24 @.@ 6 points , 8 @.@ 9 rebounds , and 3 @.@ 4 assists and led the Mavericks to a franchise @-@ high 67 wins , which meant Dallas earned the first seed of the 2007 NBA Playoffs . He averaged 50 % from the field , 40 % for three pointers , and 90 % from the free throw line becoming , at the time , only the fourth player in NBA history to join the 50 – 40 – 90 club . Nowitzki was touted as the overwhelming favorite for the Most Valuable Player award , and was expected to lead the Mavericks to an easy win against the eighth @-@ seed Golden State Warriors , despite the Warriors having won all three regular @-@ season meetings against Dallas . However , the Mavericks ended up losing to the Warriors in six games , marking the first time a # 8 seed had beaten the # 1 in a best @-@ of @-@ seven series in NBA history . In the clinching Game 6 , Nowitzki shot just 2 – 13 from the field for only eight points . Defended by Stephen Jackson , Nowitzki averaged nearly five points less than his regular @-@ season average in that series and shot 38 @.@ 3 % from the field as compared to 50 @.@ 2 % during the regular season . He described that loss as a low point in his career : \" This series , I couldn \\'t put my stamp on it the way I wanted to . That \\'s why I \\'m very disappointed . \" In spite of this historic playoffs loss , Nowitzki was named the NBA \\'s regular @-@ season Most Valuable Player and beat his friend and back @-@ to @-@ back NBA MVP Nash with more than 100 votes . He also became the first European player in NBA history to receive the honor . 2007 – 08 season The 2007 – 08 campaign saw another first @-@ round playoffs exit for Nowitzki and his Mavericks . Despite a mid @-@ season trade that sent veteran NBA All @-@ Star Jason Kidd to Dallas , the Mavericks finished seventh in a highly competitive Western Conference . Nowitzki averaged 23 @.@ 6 points , 8 @.@ 6 rebounds , and a career @-@ high 3 @.@ 5 assists for the season . In the playoffs , they faced rising star Chris Paul \\'s New Orleans Hornets , and were eliminated in five games . The playoff loss led to the firing of Avery Johnson as head coach and the eventual hiring of Rick Carlisle . The few positive highlights that season for Nowitzki were his first career triple @-@ double against the Milwaukee Bucks on February 6 , 2008 , with 29 points , 10 rebounds , and a career @-@ high 12 assists , and on March 8 , 2008 ( 34 points against the New Jersey Nets ) , when he surpassed Rolando Blackman with his <unk> point to become the Mavericks \\' all @-@ time career points leader . 2008 – 09 season The 2008 – 09 NBA season saw Nowitzki finish with averages of 25 @.@ 9 points , 8 @.@ 4 rebounds , and 2 @.@ 4 assists . He was fourth in the league in scoring , and garnered his fourth All @-@ NBA First Team selection . He also made the 2009 All @-@ Star game , his eighth appearance . Nowitzki led Dallas to a tight finish towards the playoffs , finishing 50 – 32 for the season ( 6th in the West ) , after a slow 2 – 7 start . In the playoffs , the German led Dallas to an upset win over long @-@ time rival San Antonio ( the third seed ) , winning the first @-@ round series , 4 – 1 . The Mavericks , however , fell short against the Denver Nuggets , 4 – 1 , in the second round , with Nowitzki averaging 34 @.@ 4 points , 11 @.@ 6 rebounds , and 4 assists in the series . 2009 – 10 season The Mavericks finished the 2009 – 10 NBA season as the second seed for the 2010 NBA Playoffs — it was their 10th consecutive season with at least 50 regular season wins . Notable additions to the squad were multiple All @-@ Stars Shawn Marion and Caron Butler , with the latter coming in the second half of the season . On January 13 , 2010 , Nowitzki became the 34th player in NBA history — and the first European — to hit the 20 @,@ 000 @-@ point milestone , while ending the regular season with averages of 25 points , 7 @.@ 7 rebounds , 2 @.@ 7 assists , and 1 block . He was selected to the 2010 All @-@ Star Game , his ninth appearance . The Mavericks faced off against San Antonio once more in the first round of the playoffs , but for the third time in four seasons , they failed to progress to the next round . Nowitzki was the only consistent player throughout the series for the Mavericks , averaging 26 @.@ 7 points per outing , while Jason Terry , second @-@ leading scorer for the Mavericks , averaged 12 @.@ 7 points per game compared to his 16 @.@ 6 regular season . Despite being a free agent , on July 5 , 2010 , Nowitzki agreed to remain with Dallas by re @-@ signing to a four @-@ year , $ 80 million deal . 2010 – 11 championship season The most significant change to the 2010 – 11 team roster was the arrival of Tyson Chandler via trade . Nowitzki was injured in the middle of the season , during which the Mavericks recorded their worst losing streak in over a decade . Nowitzki finished the regular season with averages of 23 points , 7 rebounds , and 3 assists . Despite missing 9 games , Nowitzki was selected to the 2011 All @-@ Star Game , his tenth appearance . The Mavericks concluded the regular season with 57 wins , seeding third behind the Spurs and Lakers for the 2011 NBA Playoffs . During the playoffs , despite their seeding , many predicted that Dallas would lose in the first round to Portland , and after blowing a 23 @-@ point fourth @-@ quarter lead in Game 4 to even the series at 2 – 2 , the Mavericks appeared ready for another postseason collapse . However , Dallas won the final two games to advance . They then swept the two @-@ time defending champion Lakers in the semifinals in Phil Jackson \\'s final year as an NBA coach . In the Conference Finals , they faced the Oklahoma City Thunder and their All @-@ NBA duo of Kevin Durant and Russell Westbrook . In Game 1 , Nowitzki scored 48 points while shooting 12 / 15 from the field while setting an NBA record of 24 consecutive free throws made in a game as well as most free throws in a game without a miss . In Game 4 , with Dallas up , 2 – 1 , Nowitzki scored 40 points to rally his team from a 99 – 84 deficit in the fourth quarter with 5 minutes left and eventually win , 112 – 105 , in OT to take a 3 – 1 series lead . Dallas overcame another fourth @-@ quarter deficit in Game 5 to win the Western Conference Title . In the 2011 NBA Finals , Dallas once again faced the Heat , which had acquired All @-@ Stars LeBron James and Chris Bosh before the season began . During the Game 1 loss in Miami , he tore a tendon in his left middle finger ; however , MRIs were negative , and Nowitzki vowed that the injury would not be a factor . In Game 2 , he led a Dallas rally from an 88 – 73 fourth @-@ quarter deficit , capped by a driving left @-@ handed layup over Bosh to tie the series at 1 . Miami took a 2 – 1 series lead after Nowitzki missed a potential game @-@ tying shot at the end of Game 3 . Despite carrying a 101 ° F fever in Game 4 , he hit the winning basket to tie the series yet again at 2 , evoking comparisons to Michael Jordan \\'s \" Flu Game \" against Utah in the 1997 NBA Finals . Dallas went on to win the next two games , with Nowitzki scoring 10 fourth @-@ quarter points in the series @-@ clinching game in Miami , bringing the first championship to the franchise . In the series , Nowitzki scored 62 points total in the six fourth quarters , equaling the combined fourth @-@ quarter output of James and Wade . He was named Finals MVP , joining a list of 10 other players to have been an NBA champion , NBA Finals MVP , an NBA regular season MVP , and a ten @-@ time All @-@ Star . For the 2011 playoffs , Nowitzki averaged 27 @.@ 7 points , 8 @.@ 1 rebounds , and 2 @.@ 5 assists in 21 games . 2011 – 12 season As Dallas celebrated their title , the NBA was in a lockout . It ended on December 8 , 2011 . The defending champions lost core players , such as DeShawn Stevenson , J.J. Barea , Peja Stojaković , and Tyson Chandler , while adding veteran all @-@ star Vince Carter , Lamar Odom , and <unk> West in free agency . The Mavericks played only two preseason games , which led to a slow start for Nowitzki . Soon , Nowitzki became the 98th player in NBA history to play in 1 @,@ 000 games . Nowitzki received his championship ring on January 25 , 2012 . After scoring his <unk> point , Nowitzki passed Robert Parish on the all @-@ time scoring list at # 20 . He then passed Charles Barkley for 19th , and scored his <unk> point on April 15 , 2012 against the Lakers . Nowitzki blocked a shot by the Celtics \\' Avery Bradley , which was his 1,000th block . He became one of only 3 players in NBA history with at least 1 @,@ 000 3 @-@ pointers and blocks , joining Clifford Robinson and Rasheed Wallace . Nowitzki made his 11th straight All @-@ Star game appearance in Orlando . Nowitzki scored 31 points in the second half against Houston on April 18 , 2012 , including 21 in the fourth quarter . It marked the third @-@ highest scoring half by a Dallas player in franchise history . The 21 points in the fourth quarter also represented Nowitzki \\'s third @-@ highest scoring quarter of his career . Carrying the load of the Mavericks , Nowitzki led his team in scoring 45 times . On March 30 , he led the Mavs down from 15 points , including a game winner , against the Magic . Nowitzki \\'s streak of 11 seasons with 1 @,@ 500 points came to an end after scoring 1 @,@ 342 in the shortened NBA season . The Mavs clinched the seventh spot in the West , and were matched against the Oklahoma City Thunder in the 2012 NBA Playoffs . The Thunder swept the Mavs in 4 games . 2012 – 13 season After failing to sign coveted free agents Deron Williams and Steve Nash , the Mavericks retooled their roster by letting go of Jason Kidd , Jason Terry and Brendan Haywood , and acquiring younger players such as O. J. Mayo , Darren Collison , as well as veteran big men Chris Kaman and Elton Brand . Nowitzki plans to play out the remainder of his 4 @-@ year , 80 million dollar contract that expires in 2014 . Nowitzki underwent knee surgery in October 2012 and missed the first 27 games of the season . He returned on December 23 , 2012 , in a game against San Antonio . In January 2013 , <unk> and some of his teammates ( including Mayo , Brand , Kaman , Carter and then teammate <unk> Jones ) made a pact not to shave their beards until the team reached .500 . They were often called \" The Beard Bros. \" On April 14 , 2013 , after a fade away jumper in a game against the New Orleans Hornets , Nowitzki became the 17th player in NBA history to score 25 @,@ 000 points . The Mavs went on to win the game , climbing back up to .500 with a 40 – 40 record and Nowitzki shaved his beard . However the Mavericks missed the playoffs for the first time since Nowitzki \\'s second season , ending their 12 @-@ year playoff streak . 2013 – 14 season At the start of the season , the Mavs let go of players they had signed the year before like O.J. Mayo , Chris Kaman , Elton Brand and Darren Collison to make room for the signing of shooting guard Monta Ellis and point guard Devin Harris . On November 12 , 2013 , in a 105 – 95 victory over the Washington Wizards , Nowitzki finished the game with 19 points and passed Jerry West on the NBA scoring list with 25 @,@ 197 points . On November 20 , 2013 , in a 123 – 120 victory over the Houston Rockets , Nowitzki finished the game with 35 points and passed Reggie Miller on the NBA scoring list with 25 @,@ 298 points . On December 23 , 2013 , in a 111 – 104 victory over the Houston Rockets , Nowitzki finished the game with 31 points and passed Alex English on the NBA scoring list with 25 @,@ 631 points . On January 29 , 2014 , Nowitzki scored his 26 @,@ 000 point in a 115 – 117 loss to the Houston Rockets . In 35 minutes of play , he recorded 38 points , 17 rebounds and 3 assists . On March 12 , 2014 , in a 108 – 101 victory over the Utah Jazz , Nowitzki finished the game with 31 points and passed John Havlicek on the NBA scoring list with 26 @,@ 426 points . On April 8 , 2014 , Nowitzki scored his <unk> point , passing Oscar Robertson to move to the 10th position on the all @-@ time scoring list . Nowitzki led the Mavericks back to the Playoffs where they would face their in state rival San Antonio Spurs in the first round . Dallas lost the series in seven games and the Spurs went on to win the NBA championship . 2014 – 15 season On July 15 , 2014 , Nowitzki re @-@ signed with the Mavericks to a reported three @-@ year , $ 25 million contract . He was also reunited with former championship teammate Tyson Chandler , who was traded to Dallas after a 3 @-@ year stint with New York . Also to come over from New York was point guard Raymond Felton . The Mavericks made big strides in the off @-@ season to put Dirk with some considerable talent , signing a rising small forward Chandler Parsons and veteran point guard Jameer Nelson . The Mavericks later signed former championship teammate J. J. Barea after the Timberwolves waived him . On November 11 , 2014 , Nowitzki scored 23 points to surpass Hakeem Olajuwon as the highest @-@ scoring player born outside the United States , as the Mavericks came from 24 points down to defeat Sacramento 106 – 98 for their 21st straight regular @-@ season win at home against the Kings . Nowitzki hit a jumper from just inside the three @-@ point line early in the fourth quarter to pass Olajuwon at No. 9 , and he finished the night at 26 @,@ 953 career points . Six days later , Nowitzki became the fourth player in NBA history to eclipse 27 @,@ 000 career points with the same franchise , joining a prestigious group that includes Michael Jordan , Karl Malone and Kobe Bryant . On December 26 against the Los Angeles Lakers , Nowitzki passed Elvin Hayes for eighth place on the NBA \\'s all @-@ time scoring list . He went on to pass Moses Malone for seventh place on the NBA \\'s all @-@ time scoring list on January 5 , 2015 in a 96 – 88 overtime win over the Brooklyn Nets . He <unk> his 10,000th career rebound on March 24 against the San Antonio Spurs , and scored his <unk> career point on April 1 against the Oklahoma City Thunder . The Mavericks finished the regular season as the No. 7 seed in the Western Conference with a record of 50 – 32 . They faced the Houston Rockets in the first round of the playoffs and lost the series in five games . 2015 – 16 season On November 11 , 2015 , Nowitzki scored a season @-@ high 31 points in a 118 – 108 win over the Los Angeles Clippers . He also grabbed a team @-@ high 11 rebounds and passed former teammate Shawn Marion for 15th on the all @-@ time career rebounding list . On December 12 , in a loss to the Washington Wizards , he moved past Jason Richardson for 16th all @-@ time in three @-@ point shots made , finishing the game with 1 @,@ 609 career three @-@ pointers . On December 23 , Nowitzki moved past Shaquille O \\'Neal into sixth place on the NBA \\'s career scoring list , then made the go @-@ ahead basket with 19 @.@ 2 seconds left in overtime to help the Mavericks defeat the Brooklyn Nets , 119 – 118 . On January 12 , in an overtime loss to the Cleveland Cavaliers , he became the 14th NBA player to reach 46 @,@ 000 career minutes . On January 18 , he tied a season high with 31 points and had 11 rebounds for his sixth double @-@ double of the season in a 118 – 113 overtime win over the Boston Celtics . On February 21 , he scored 18 points against the Philadelphia 76ers , becoming the sixth player in NBA history to reach 29 @,@ 000 career points . Three days later , he set a new season high with 33 points in a loss to the Oklahoma City Thunder . On March 20 , he set another season high mark with 40 points in a 132 – 120 overtime win over the Portland Trail Blazers . His 20th career 40 @-@ point game was his first since January 2014 , and the first by a 37 @-@ year @-@ old since Karl Malone in 2000 – 01 . On April 11 , in a win over the Utah Jazz that clinched the Mavericks a spot in the playoffs for the 15th time in 16 years , Nowitzki hit four three @-@ pointers , becoming the 15th player in NBA history with at least 1 @,@ 700 career triples . In Game 4 of the Mavericks \\' first @-@ round playoff series against the Oklahoma City Thunder , Nowitzki passed Elgin Baylor ( 3 @,@ 623 points ) for 15th on the NBA \\'s career playoff scoring list . 2016 – 17 season On July 22 , 2016 , Nowitzki agreed to re @-@ sign with the Mavericks on a two @-@ year , $ 50 million contract .']\n",
      "['Books , as editor Marxism and Native Americans . Boulder CO : South End Press . 1984 . ISBN 978 @-@ 0 @-@ <unk> @-@ 178 @-@ 9 . Sharon <unk> , ed . ( 1997 ) . Islands in Captivity : The International Tribunal on the Rights of Indigenous Hawaiians . Boulder CO : South End Press . ISBN 978 @-@ 0 @-@ <unk> @-@ 568 @-@ 8 . Re @-@ released as Churchill , Ward ( 2005 ) . Sharon <unk> , ed . Islands in Captivity : The Record of the International Tribunal on the Rights of Indigenous Hawaiians . Boulder CO : South End Press . ISBN 978 @-@ 0 @-@ <unk> @-@ 738 @-@ 5 . Natsu Saito , ed . ( 2006 ) . Confronting The Crime Of Silence : Evidence Of U.S. War Crimes In Indochina . AK Press . ISBN 978 @-@ 1 @-@ <unk> @-@ 21 @-@ 5 . Books , as author and co @-@ author : Culture versus <unk> : Essays on Marxism in the Multicultural <unk> with Elisabeth Lloyd . <unk> Press . 1984 . Agents of Repression : The FBI \\'s Secret Wars Against the Black Panther Party and the American Indian Movement. with Jim Vander Wall . Boulder CO : South End Press . 1988 . ISBN 978 @-@ 0 @-@ <unk> @-@ 294 @-@ 6 . The COINTELPRO Papers : Documents from the FBI \\'s Secret War Against Domestic <unk> with Jim Vander Wall . Boulder CO : South End Press . 1990 . ISBN 978 @-@ 0 @-@ <unk> @-@ 359 @-@ 2 . Fantasies of the Master Race : Literature , Cinema , and the Colonization of American Indians . Common Courage Press . 1992 . ISBN 978 @-@ 0 @-@ <unk> @-@ 348 @-@ 4 . Churchill , Ward ( 1992 ) . Jennie and Jim Vander Wall , ed . Cages of Steel : The Politics of Imprisonment in America ( Activism , Politics , Culture , Theory , Vol . 4 ed . ) . Maisonneuve Press . ISBN 978 @-@ 0 @-@ <unk> @-@ 17 @-@ 3 . Re @-@ released as Churchill , Ward ( 2004 ) . Jim Vander Wall , ed . Politics of Imprisonment in the United States . AK Press . ISBN 978 @-@ 1 @-@ <unk> @-@ 12 @-@ 3 . Struggle for the Land : Indigenous Resistance to Genocide , <unk> and <unk> in Contemporary North America . Common Courage Press . 1993 . ISBN 978 @-@ 1 @-@ <unk> @-@ 001 @-@ 0 . Revised and expanded edition : Struggle for the Land : Native North American Resistance to Genocide , <unk> and Colonization . San Francisco CA : City Lights Books . 2002 . ISBN 978 @-@ 0 @-@ <unk> @-@ 415 @-@ 3 . Indians Are Us ? : Culture and Genocide in Native North America . Common Courage Press . 1994 . ISBN 978 @-@ 1 @-@ <unk> @-@ 021 @-@ 8 . Since Predator Came : Notes from the Struggle for American Indian Liberation . Aigis Press . 1995 . ISBN 978 @-@ 1 @-@ <unk> @-@ 03 @-@ 5 . Churchill , Ward ( 1996 ) . From a Native Son : Selected Essays on <unk> 1985 – 1995 . Boulder CO : South End Press . ISBN 978 @-@ 0 @-@ <unk> @-@ 553 @-@ 4 . Pacifism as Pathology : Reflections on the Role of Armed Struggle in North America. with Mike Ryan ( introduction by Ed Mead ) . Arbeiter Ring . 1998 . ISBN 978 @-@ 1 @-@ <unk> @-@ 07 @-@ 5 . A Little Matter of Genocide . San Francisco CA : City Lights Books . 1998 . ISBN 978 @-@ 0 @-@ <unk> @-@ 343 @-@ 9 . Draconian Measures : The History of FBI Political Repression . Common Courage Press . 2000 . ISBN 978 @-@ 1 @-@ <unk> @-@ 059 @-@ 1 . Acts Of Rebellion : The Ward Churchill Reader . Routledge . 2002 . ISBN 978 @-@ 0 @-@ 415 @-@ <unk> @-@ 4 . <unk> of Justice : Indigenous Peoples and <unk> Law . San Francisco CA : City Lights Books . 2002 . ISBN 978 @-@ 0 @-@ <unk> @-@ 416 @-@ 0 . On the Justice of Roosting Chickens : Reflections on the Consequences of U.S. Imperial Arrogance and <unk> . AK Press . 2003 . ISBN 978 @-@ 1 @-@ <unk> @-@ 79 @-@ 1 . Kill the Indian , Save the Man : The Genocidal Impact of American Indian Residential Schools . San Francisco CA : City Lights Books . 2004 . ISBN 978 @-@ 0 @-@ <unk> @-@ 434 @-@ 4 . Speaking Truth in the Teeth of Power : Lectures on Globalization , Colonialism , and Native North America . AK Press . 2004 . ISBN 978 @-@ 1 @-@ <unk> @-@ 04 @-@ 8 . To <unk> , <unk> And Destroy : The FBI \\'s Secret War Against The Black Panther Party . Routledge . 2005 . ISBN 978 @-@ 0 @-@ 415 @-@ <unk> @-@ 8 . Articles Churchill , Ward ( July – September 1992 ) . \" I Am <unk> : Notes on the Ideology of the Fourth World \" . Z Papers 1 ( 3 ) . Archived from the original on October 14 , 2007 . Churchill , Ward ( 1994 ) . \" Let \\'s Spread the Fun Around \" . First published as \" Crimes Against Humanity \" in Margaret Anderson and Patricia Hill ( eds . ) ( 1994 ) . Race , Class and Gender : An Anthology . Belmont , CA : <unk> pp. 366 – 73 . Also published under the titles \" The Indian Chant and the Tomahawk Chop \" and \" Using Indian Names as Mascots Harms Native Americans \" . Churchill , Ward ( November 1998 ) . \" Smoke Signals : A History of Native Americans in Cinema \" . <unk> Magazine . Churchill , Ward ( Winter – Spring 2003 ) . \" An American Holocaust ? The Structure of Denial \" . Socialism and Democracy 17 ( 2 ) : 25 – 76 @.@ doi : 10 @.@ 1080 / <unk> . Churchill , Ward ( Spring 2005 ) . \" The Ghosts of 9 @-@ 1 @-@ 1 : Reflections on History , Justice and Roosting Chickens \" . Alternative Press Review 9 ( 1 ) : 45 – 56 . Archived from the original on October 2 , 2006 . Churchill , Ward ( July – August 2007 ) . \" The Fourth World : Struggles for Traditional Lands and Ways of Life \" . Left Turn 25 : 25 – 29 . Audio and video Doing Time : The Politics of Imprisonment , audio CD of a lecture , recorded at the Doing Time Conference at the University of Winnipeg , September 2000 ( AK Press , 2001 , ISBN 978 @-@ 1 @-@ <unk> @-@ 47 @-@ 0 ) Life In Occupied America ( AK Press , 2003 , ISBN 978 @-@ 1 @-@ <unk> @-@ 72 @-@ 2 ) In A Pig \\'s Eye : Reflections on the Police State , Repression , and Native America ( AK Press , 2002 , ISBN 978 @-@ 1 @-@ <unk> @-@ 50 @-@ 0 ) US Off The Planet ! : An Evening In Eugene With Ward Churchill And <unk> Glendinning , VHS video recorded July 17 , 2001 ( Cascadia Media Collective , 2002 ) Pacifism and Pathology in the American Left , 2003 audio CD recorded at an AK Press warehouse in Oakland ( AK Press Audio ) Z Mag Ward Churchill Audio August 10 , 2003 and earlier Churchill Speaks About Academic Freedom – Free Speech Radio News February 9 , 2005 Ward Churchill Under Fire – Free Speech Radio News , February 3 , 2005 . The Justice of Roosting Chickens : Ward Churchill Speaks The Pacifica Network Show , Democracy Now ! from February 18 , 2005 features extended Audio / Video exclusive interview with Churchill . A Little Matter of Genocide : Linking U.S. Aggression Abroad to the Domestic Repression of Indigenous Peoples \" , recorded in North Battleford , Saskatchewan on March 19 , 2005 Debate with David Horowitz and Ward Churchill at George Washington University April 6 , 2006 \" Full two @-@ hour audio of debate with David Horowitz \" . <unk> Retrieved 2006 @-@ 07 @-@ 02 . \" David Horowitz vs. Ward Churchill — Round 1 \" . Young Americans Foundation . Retrieved 2006 @-@ 07 @-@ 02 . Video and audio ( excerpt ) \" David Horowitz vs. Ward Churchill \" . <unk> Retrieved 2006 @-@ 07 @-@ 02 .']\n"
     ]
    }
   ],
   "source": [
    "print([tokenizer.decode(train_tokens[-5])])\n",
    "print([tokenizer.decode(val_tokens[-5])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "279cdacc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_idx = tokenizer.encode(\"<PAD>\").ids[0]\n",
    "pad_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b2cc5d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(sentence):\n",
    "    sos_idx = tokenizer.encode('<SOS>').ids[0]\n",
    "    eos_idx = tokenizer.encode('<EOS>').ids[0]\n",
    "\n",
    "    return [sos_idx] + sentence + [eos_idx]\n",
    "\n",
    "def decode(sentence):\n",
    "    return tokenizer.decode(list(sentence.detach().cpu()))\n",
    "\n",
    "def add_padding(X, y, pad_idx, batch_size=32):\n",
    "    num_sentences = len(X)\n",
    "    for i in range(0, num_sentences, batch_size):\n",
    "        idx = min(num_sentences-1, i + batch_size-1)\n",
    "        max_length = len(X[idx])\n",
    "        for j in range(i, idx+1):\n",
    "            missing_X = max_length - len(X[j])\n",
    "            \n",
    "            X[j].extend([pad_idx]*missing_X)\n",
    "            y[j].extend([pad_idx]*(missing_X + 1))\n",
    "\n",
    "    return X, y\n",
    "\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.long), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Train dataloader\n",
    "train_tokens = [process(tokens) for tokens in train_tokens]\n",
    "y_tokens = [tokens[1:] for tokens in train_tokens]\n",
    "X_train, y_train = add_padding(train_tokens, y_tokens, pad_idx, batch_size)\n",
    "train_dataset = LMDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Val dataloader\n",
    "val_tokens = [process(tokens) for tokens in val_tokens]\n",
    "y_tokens = [tokens[1:] for tokens in val_tokens]\n",
    "X_val, y_val = add_padding(val_tokens, y_tokens, pad_idx, batch_size)\n",
    "val_dataset = LMDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6ed515ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 30000\n",
      "\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16, 4])\n",
      "\n",
      "torch.Size([16, 35])\n",
      "torch.Size([16, 35])\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(tokenizer.get_vocab())\n",
    "print(f\"Size of vocabulary: {vocabulary_size}\\n\")\n",
    "\n",
    "for X, y in train_loader:\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    break\n",
    "\n",
    "print()\n",
    "\n",
    "for X, y in val_loader:\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710e92f5",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9c827a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAttentionHead(nn.Module):\n",
    "    def __init__(self, num_heads, hidden_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        head_dim = hidden_size // num_heads\n",
    "\n",
    "        # Weight matrices for queries, keys, values\n",
    "        self.W_q = nn.Parameter(torch.randn(num_heads, hidden_size, head_dim))\n",
    "        self.W_k = nn.Parameter(torch.randn(num_heads, hidden_size, head_dim))\n",
    "        self.W_v = nn.Parameter(torch.randn(num_heads, hidden_size, head_dim))\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.head_dim = head_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Store the key and values\n",
    "        self.past_key = None \n",
    "        self.past_value = None \n",
    "\n",
    "        # Output linear projection\n",
    "        self.output_proj = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
    "\n",
    "        # Normalization layer\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # Attention dropout\n",
    "        self.attention_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):        \n",
    "        std = 0.02  \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.W_q.normal_(0, std)\n",
    "            self.W_k.normal_(0, std) \n",
    "            self.W_v.normal_(0, std)\n",
    "            self.output_proj.normal_(0, std)\n",
    "\n",
    "    def forward(self, X, padding_mask, auto_reg=False):\n",
    "        \"\"\"\n",
    "        Forward pass for multi-head attention.\n",
    "\n",
    "        Inputs:\n",
    "            X            : (batch_size, seq_len, hidden_size)\n",
    "            padding_mask : (batch_size, seq_len)  -> 1 for real tokens, 0 for padding\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = X.shape\n",
    "        device = X.device \n",
    "        X_heads = X.unsqueeze(1)\n",
    "\n",
    "        if not auto_reg:\n",
    "            # Create padding mask\n",
    "            mask_matrix = padding_mask.unsqueeze(-1) * padding_mask.unsqueeze(-2)\n",
    "            pad_mask = mask_matrix.unsqueeze(1).expand(-1, self.num_heads, -1, -1)\n",
    "\n",
    "            # Create causal mask\n",
    "            causal_mask = torch.tril(torch.ones((seq_len, seq_len), device=device))\n",
    "            causal_mask = causal_mask.unsqueeze(0).unsqueeze(0).expand(batch_size, self.num_heads, -1, -1)\n",
    "\n",
    "            # Combine masks\n",
    "            att_mask = pad_mask * causal_mask\n",
    "\n",
    "            # Add head dimension to input\n",
    "            X_heads = X.unsqueeze(1)  # (batch, 1, seq_len, hidden_size)\n",
    "\n",
    "            # Compute Q, K, V for full sequence\n",
    "            Q = torch.matmul(X_heads, self.W_q)  # (batch, num_heads, seq_len, head_dim)\n",
    "            K = torch.matmul(X_heads, self.W_k)  # (batch, num_heads, seq_len, head_dim)\n",
    "            V = torch.matmul(X_heads, self.W_v)  # (batch, num_heads, seq_len, head_dim)\n",
    "            \n",
    "        else:\n",
    "    \n",
    "            # Compute Q for current token only\n",
    "            Q = torch.matmul(X_heads, self.W_q)  # (batch, num_heads, 1, head_dim)\n",
    "            \n",
    "            if self.past_key is None:\n",
    "                # First token - compute K,V for the entire sequence so far\n",
    "                K = torch.matmul(X_heads, self.W_k)\n",
    "                V = torch.matmul(X_heads, self.W_v)\n",
    "            \n",
    "            else:\n",
    "                # Subsequent tokens - compute K,V only for current token and concatenate\n",
    "                curr_K = torch.matmul(X_heads, self.W_k)  # (batch, num_heads, 1, head_dim)\n",
    "                curr_V = torch.matmul(X_heads, self.W_v)\n",
    "                \n",
    "                # Concatenate with cached values\n",
    "                K = torch.cat([self.past_key, curr_K], dim=2)\n",
    "                V = torch.cat([self.past_value, curr_V], dim=2)\n",
    "\n",
    "            self.past_key = K\n",
    "            self.past_value = V\n",
    "\n",
    "\n",
    "        # Compute attention scores\n",
    "        K_transposed = K.transpose(-2, -1)  # (batch, num_heads, head_dim, kv_seq_len)\n",
    "        scores = torch.matmul(Q, K_transposed)  # (batch, num_heads, q_seq_len, kv_seq_len)\n",
    "        scaled_scores = scores / (self.head_dim ** 0.5)\n",
    "\n",
    "        # Apply mask and softmax\n",
    "        LARGE_NEG = -1e9\n",
    "        if not auto_reg:\n",
    "            scaled_scores = scaled_scores.masked_fill(~(att_mask == 1), LARGE_NEG)\n",
    "        \n",
    "        att_weights = torch.softmax(scaled_scores, dim=-1)\n",
    "        att_weights = self.attention_dropout(att_weights)\n",
    "\n",
    "        # Attention output\n",
    "        att_output = torch.matmul(att_weights, V)  # (batch, num_heads, q_seq_len, head_dim)\n",
    "        \n",
    "        if auto_reg:\n",
    "            combined_heads = att_output.reshape(batch_size, 1, self.hidden_size)\n",
    "        else:\n",
    "            combined_heads = att_output.reshape(batch_size, seq_len, self.hidden_size)\n",
    "\n",
    "        # Apply layer normalization and the final projection\n",
    "        norm_output = self.norm(combined_heads)\n",
    "        projected_output = torch.matmul(norm_output, self.output_proj)\n",
    "\n",
    "        return projected_output\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        self.past_key, self.past_value = None, None\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, hidden_layers=2, act='relu', hidden_size=[32, 32], input_size=32, dropout=0.1):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(input_size, hidden_size[0])]\n",
    "\n",
    "        for i in range(1, hidden_layers):\n",
    "            if act == 'relu':\n",
    "                layers.append(nn.ReLU())\n",
    "\n",
    "            elif act == 'tanh':\n",
    "                layers.append(nn.Tanh())\n",
    "            \n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            layers.append(nn.Linear(hidden_size[i-1], hidden_size[i]))\n",
    "            \n",
    "                \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_seq_len, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "        position = torch.arange(max_seq_len).unsqueeze(1) # (seq_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2) * (-math.log(10000.0) / embed_size))\n",
    "        pe = torch.zeros(1, max_seq_len, embed_size)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term) # (1, max_seq_len, embed_size)\n",
    "        self.register_buffer('pe', pe) # So it goes to CPU/GPU as well\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            X (batch_size, seq_len, embed_size)\n",
    "        \"\"\"\n",
    "        X = X + self.pe[:, :X.shape[1],:]\n",
    "        return self.dropout(X)  \n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, voc_size, embed_size, num_heads, depth, pad_idx, p=0.5):\n",
    "        super().__init__()\n",
    "        self.voc_size = voc_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = depth\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        # Embeddings layer\n",
    "        self.embed = nn.Embedding(voc_size, embed_size)\n",
    "        nn.init.xavier_uniform_(self.embed.weight)\n",
    "        self.embed_dropout = nn.Dropout(p*0.5)\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.pe = PositionalEncoding(embed_size=self.embed_size, max_seq_len=2050, dropout=p).to(device)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "        # Attention Heads\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                MultiAttentionHead(num_heads, embed_size),\n",
    "                nn.LayerNorm(embed_size),\n",
    "                FFN(hidden_layers=2, act=\"relu\",\n",
    "                    hidden_size=[4 * embed_size, embed_size],\n",
    "                    input_size=embed_size, dropout=p*0.8),\n",
    "                nn.LayerNorm(embed_size)\n",
    "            ])\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        # Last linear layer\n",
    "        self.final_dropout = nn.Dropout(p*0.5)\n",
    "        self.linear = nn.Linear(embed_size, voc_size, bias=False)\n",
    "    \n",
    "\n",
    "    def forward(self, X, auto_reg=False):\n",
    "        '''\n",
    "        Inputs:\n",
    "            X (batch_size, seq_length)\n",
    "        '''\n",
    "        _, seq_len = X.shape\n",
    "        padding_mask = X != self.pad_idx\n",
    "\n",
    "        # Positional Encoding\n",
    "\n",
    "        X = self.embed(X) # (batch_size, seq_length, embedding_size)\n",
    "        X = self.embed_dropout(X)\n",
    "        X = self.pe(X)\n",
    "\n",
    "\n",
    "        for head, norm1, ffn, norm2 in self.heads:\n",
    "            # Go through the multi heads\n",
    "            X = self.dropout(norm1(head(X, padding_mask, auto_reg) + X))\n",
    "\n",
    "            # Do the same for the FFN\n",
    "            X = self.dropout(norm2(ffn(X) + X))\n",
    "        \n",
    "        X = self.final_dropout(X)\n",
    "        output = self.linear(X)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def generate_from_prompt(self, inputs, sample='p', max_size=1000):\n",
    "        self.eval() # We aren't training anymore\n",
    "        self.clear_memo() # Clear hashes\n",
    "        curr = len(inputs)\n",
    "        eos_idx = tokenizer.encode(\"<EOS>\").ids[0]\n",
    "        while curr < max_size:\n",
    "            X = torch.tensor(inputs[-1], dtype=torch.long).unsqueeze(0).unsqueeze(1).to(device) # Add batch size 1\n",
    "            with torch.no_grad():\n",
    "                logits = self(X, auto_reg=True)\n",
    "        \n",
    "            # Sample from logits\n",
    "            match sample:\n",
    "                case 'k':\n",
    "                    token = self.top_k_sample(logits)[0, 0].item()\n",
    "                \n",
    "                case 'greedy':\n",
    "                    token = self.greedy_sample(logits)[0, 0].item()\n",
    "                \n",
    "                case 'p':\n",
    "                    token = self.top_p_sample(logits)[0, 0].item()\n",
    "        \n",
    "\n",
    "            # See if end of sentence\n",
    "            if token == eos_idx:\n",
    "                break\n",
    "            \n",
    "            curr += 1\n",
    "            inputs.append(token)\n",
    "        \n",
    "        return tokenizer.decode(inputs)\n",
    "\n",
    "    def clear_memo(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, MultiAttentionHead):\n",
    "                module.clear_cache()\n",
    "\n",
    "\n",
    "    def greedy_sample(self, output):\n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(output, dim=-1)\n",
    "\n",
    "\n",
    "    def top_k_sample(self, output, k=50):\n",
    "        '''\n",
    "        Inputs:\n",
    "            output (batch_size, seq_len, voc_size)\n",
    "        '''\n",
    "        batch_size, seq_len, _ = output.shape \n",
    "        res = torch.zeros((batch_size, seq_len), dtype=torch.long).to(device)\n",
    "        with torch.no_grad():\n",
    "            probabilities = torch.softmax(output, dim=-1)\n",
    "            values, indices = torch.topk(probabilities, k=k, dim=-1)\n",
    "            batch_idx = torch.arange(batch_size)\n",
    "            for pos in range(seq_len): \n",
    "                sample = torch.multinomial(values[:, pos, :], num_samples=1).squeeze(1) # (batch_size)\n",
    "                res[:, pos] = indices[batch_idx, pos, sample]\n",
    "        \n",
    "        return res\n",
    "\n",
    "    def top_p_sample(self, output, p=0.9):\n",
    "        '''\n",
    "        Inputs:\n",
    "            output (batch_size, seq_len, voc_size)\n",
    "        '''\n",
    "        batch_size, seq_len, _ = output.shape\n",
    "        res = torch.zeros((batch_size, seq_len), dtype=torch.long).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probs = torch.softmax(output, dim=-1)  # (batch, seq_len, vocab)\n",
    "            batch_idx = torch.arange(batch_size)\n",
    "            sorted_probs, sorted_idx = torch.sort(probs, descending=True, dim=-1)\n",
    "            cum_probs = sorted_probs.cumsum(dim=-1)\n",
    "            mask = cum_probs > p\n",
    "            mask[..., 1:] = mask[..., :-1].clone()  # keep at least first token\n",
    "            sorted_probs[mask] = 0.0\n",
    "            sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
    "            for pos in range(seq_len):\n",
    "                sample = torch.multinomial(sorted_probs[:, pos, :], num_samples=1).squeeze(-1)\n",
    "                res[:, pos] = sorted_idx[batch_idx, pos, sample]\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f4297511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: torch.Size([2, 5, 512])\n",
      "torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "# Example dimensions\n",
    "batch_size = 2    # number of sequences\n",
    "seq_len = 5       # tokens per sequence\n",
    "hidden_size = 512   # embedding dimension\n",
    "h = 8 # number of heads\n",
    "head = MultiAttentionHead(h, hidden_size).to(device)\n",
    "\n",
    "# Create random input\n",
    "X = torch.randn(batch_size, seq_len, hidden_size).to(device)\n",
    "att_mask = (torch.randint(0, 2, (batch_size, seq_len)) == 1).to(device)\n",
    "output = head(X, att_mask)\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dcc3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(voc_size=vocabulary_size, embed_size=256, num_heads=4, depth=6,\\\n",
    "                    pad_idx=pad_idx, p=0.35).to(device)\n",
    "\n",
    "state = torch.load(\"models/overall_best.pth\", map_location=device, weights_only=True)\n",
    "transformer.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "94ed49d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello================================================= (= (==== A================================in================= World====================== Leaguees :== 1 @-@ II=== <= (D (1 Newth : Cup World York (C=ers LeagueM 1 (C ( 1 ' Man City :C @-@ ( @-@ ( ' ,= 1S Ocean.E 2age 3 I 2 ( ( ( ( fromesTS 1A :S with 1M \"MingS2 ( adaptedB album.F adapted> notes @-@ fromA \" notes \" 1C adapted> adaptedS 2 1 3 ) adapted isFs from )WS from fromS 1 adaptedsunk adapted adapted> album adapted> States and career adapted> from : notes> notes adapted adapted from notes ' notes : notes adapted>s adapted from from> : from adapted adapteds for creditsCM : where wereB taken : :unk adapted notesunk ) and notes notes : adapted – adaptedFF adapted adapted taken adapted – adapted were eraA era adaptedM : goalsCs adapted : : for written adapted 2011 and adapteds 1994s album. adapted : adapted> single : lifesats : adapted adapted CDs adapted adapted areM CD tracksats adapted : from : notes : notes statistics byMunkunk : notes notes adaptedMunk from from adapted adapteds adapted 1994 adaptedats are taken :BS adapted where :F by notes careerW adapted adapted adapted. notes statistics album CD credits : adapted adapted :unk taken : adapted2 by notesAMF : adapted 'I adapted tracks album :A from taken ' adaptedatsatsA adapted from. for fromD CDats adapted. for life notes written adapted adapted from adaptedM ' '. where noted from results forM album is adapted : adapted adapted adapted isLW adaptedS adapted are : are CD results. albumsA ' adapted wheres and by notes adapted album notes notes notes takenats adapted :A adaptedF adapted taken writtens writtens from iss. goal : adaptedM adaptedunk adapted.S adaptedMunk :F adapted notesA album : : credits. careerats where credits is written album ' credits : adapted adapted and adaptedA are 2011 where. noted notes :C takenI : noted adapted adapted and adapted are adapted for adapted :ats life and written from adaptedS )C album adaptedunk : notes areS from adapted adapted except adapted are notes adaptedSC notes adaptedats notes career credits adapted except notesA notes notes adaptedA CD>C credits adapted adapted adapted notes adapted adaptedatss adaptedA adaptedM adapted notes from notes noted chartS adapted awards :M : career notesits adapted taken tracks career adapted. statisticsits wheresW2s.s2 ) where adapted. ) tracks : awards productionats adapted chartC notes :s written album :M adapted : adapteds taken credits notes era :F are : : awardsS adapteds : adapted is adapted adaptedL taken adapted are forats life adapted )s and album adapted notes where adapted taken taken albumunk : careers. career is notesats ) : career : adapted CD adaptedats notesS album adapted adapted adaptedAS adapted notes adapted adapted taken adapted adapted adapted adapted : adapted where production careerS adaptedS adapted adapted : – ) are adapted adapted adapted adapted2 adapted : adapted for notes taken are :> adapted creditsA era taken single : : adapted fromF eras takenM adapted written written adapted career career album tracks tracks adapted adaptedunk careerF by :FSWunkunk are taken : notes adapted 2011 taken :. career adapted notes written from adaptedA byunk ) adapted adapted adapted adapted adapted adaptedats taken AllMusic for league notesats adapted are resultsitsI tracks : adapted adapted where 2011 : adapted adapted taken adapted adapted forL CDL notes adapted notesM adapted taken credits from : from tracks adapted notes adaptedunk adapted music taken albumFWI are taken adapted productionA written results taken where for adapteds league adapted ) adapted adaptedIS statistics : tracks notes ' CD from noted adapted where for adapted 2011 credits adapted adapted are adapted : adapted adapted results adapted adaptedats notes adapted adapted adaptedM notes are for lifeI adapted adapted froms album notes notes notes where adapted whereunk adaptedA adapted adaptedats ' adapted lifeL notesM ' : taken : League : ares notes : : adapted are. from notes adaptedF :S tracksA careerA adapted\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Hello\"\n",
    "ans = transformer.generate_from_prompt(tokenizer.encode(prompt).ids, sample='k')\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "6b0325a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape of k sampling: torch.Size([10, 4])\n",
      "Output shape of greedy sampling: torch.Size([10, 4])\n",
      "Output shape of greedy sampling: torch.Size([10, 4])\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(voc_size=vocabulary_size, embed_size=256, num_heads=4, depth=6,\\\n",
    "                    pad_idx=pad_idx, p=0.35).to(device)\n",
    "X_test = torch.tensor(X_train[:10]).to(device)\n",
    "#transformer = Transformer(voc_size=len(word2idx), embed_size=512, num_heads=8, depth=12,\\\n",
    "                          #pad_idx=word2idx['<PAD>'], p=0.3).to(device)\n",
    "\n",
    "output = transformer(X_test)\n",
    "\n",
    "prediction_k = transformer.top_k_sample(output)\n",
    "prediction_greedy = transformer.greedy_sample(output)\n",
    "prediction_p = transformer.top_p_sample(output)\n",
    "print(f\"Output shape of k sampling: {prediction_k.shape}\")\n",
    "print(f\"Output shape of greedy sampling: {prediction_greedy.shape}\")\n",
    "print(f\"Output shape of greedy sampling: {prediction_p.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "fe450245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text for p sampling:\n",
      " Honorary skateiating cartoons\n",
      " computer Bills canal vaccine\n",
      " cease Single Cumberland Kan\n",
      " Fair interrogation cabin Bott\n",
      " endorsed Brooklyn stretches conceived\n",
      " Palestinianrain dissem Helms\n",
      " selectors Infantry lacrosseeler\n",
      " Gross Luther dream Am\n",
      " proving Notableraft Crime\n",
      " Sheila pursuing carriers appealing\n",
      "\n",
      "Text for k sampling:\n",
      " Digital activation diving frame\n",
      " contributedcast Bisc ign\n",
      "Station territories tie]\n",
      " contrasting communication Crime non\n",
      "ront Gentlemen feet Ves\n",
      " Strong martyr older insuff\n",
      " Foster Cal Shap Walton\n",
      " narrow Nos ways Charleston\n",
      " fundalandlow Alpha\n",
      " Gwyn Ricardo Basin Box\n",
      "\n",
      "Text for greedy sampling:\n",
      " Communicationshesis awarding Nag\n",
      " Dubrovnikoor casino Cross\n",
      " scandal Syndicate placuct\n",
      " Alleghenyedom adopted parasitic\n",
      " 39 rebound sabot Extra\n",
      "tymier Picture singer\n",
      "oderm step Shap Karab\n",
      " 81�mental organization\n",
      " Carlos pair countryinge\n",
      " frequency retros weighed Farrell\n"
     ]
    }
   ],
   "source": [
    "print(\"Text for p sampling:\")\n",
    "for batch in prediction_p:\n",
    "    print(decode(batch))\n",
    "\n",
    "print(\"\\nText for k sampling:\")\n",
    "for batch in prediction_k:\n",
    "    print(decode(batch))\n",
    "\n",
    "print(\"\\nText for greedy sampling:\")\n",
    "\n",
    "for batch in prediction_greedy:\n",
    "    print(decode(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "0eeb4698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, optimizer, scheduler, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for i, (batch_X, batch_y) in enumerate(train_loader):\n",
    "        if (i / len(train_loader)) > 0.5:\n",
    "            break\n",
    "            print(f\"{(i / len(train_loader)) * 100:.2f}% done\")\n",
    "            print(total_loss)\n",
    "            print(total_loss/(i+1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device) # (batch_size, seq_len)\n",
    "        \n",
    "        outputs = model(batch_X) # (batch_size, seq_len, voc_size)\n",
    "        outputs = outputs.permute(0, 2, 1) # (batch_size, voc_size, seq_len) -> expected size for nn.CrossEntropy\n",
    "\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() \n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def evaluate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_X, batch_y) in enumerate(val_loader):\n",
    "            if i % 3000 == 0 or i == len(train_loader) - 1:\n",
    "                print(f\"{(i / len(train_loader)) * 100:.2f}% done\")\n",
    "\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device) # (batch_size, seq_len)\n",
    "            \n",
    "            outputs = model(batch_X) # (batch_size, seq_len, voc_size)\n",
    "            outputs = outputs.permute(0, 2, 1) # (batch_size, voc_size, seq_len) -> expected size for nn.CrossEntropy\n",
    "\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            total_loss += loss.item() \n",
    "\n",
    "    return total_loss / len(val_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "42374506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_transformer_training(model, total_steps, warmup_steps):\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=1e-4,             \n",
    "        weight_decay=0.1,\n",
    "        betas=(0.9, 0.98)\n",
    "    )\n",
    "\n",
    "    # Learning rate schedule with warmup + linear decay to zero\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lr_lambda=lambda step: min((step + 1) / warmup_steps, \n",
    "                                   max(0.0, (total_steps - step) / max(1, total_steps - warmup_steps)))\n",
    "    )\n",
    "\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2043a632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Epoch 1\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "warmup_epochs = 5\n",
    "patience = 5\n",
    "model = Transformer(voc_size=vocabulary_size, embed_size=128, num_heads=4, depth=4,\\\n",
    "                    pad_idx=pad_idx, p=0.6).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx, label_smoothing=0.05)\n",
    "\n",
    "opt, scheduler = setup_transformer_training(model, num_epochs, warmup_epochs)\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_model_state = None\n",
    "curr = 0\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    print(\"-\"*50)\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    train_loss = train_one_epoch(model, train_loader, opt, scheduler, criterion)\n",
    "    val_loss = evaluate(model, val_loader, criterion)\n",
    "    print(f\"Train Loss: {train_loss:.5f}\")\n",
    "    print(f\"Val Loss: {val_loss:.5f}\")\n",
    "    \n",
    "    # Early stopping logic\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        print(f\"Found better model at epoch {epoch}. Saving...\")\n",
    "        torch.save(best_model_state, \"models/best_model.pth\")\n",
    "        curr = 0\n",
    "    else:\n",
    "        curr += 1\n",
    "        if curr == patience:\n",
    "            print(\"No more patience. Ending Training Loop...\")\n",
    "            break\n",
    "\n",
    "    print(f\"Best Val Loss: {best_loss:.5f}\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "# If it ends the training loop\n",
    "torch.save(best_model_state, \"models/overall_best.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

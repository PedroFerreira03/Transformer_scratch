{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ae530d5",
   "metadata": {},
   "source": [
    "# Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d210ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b656171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt_tab') -> Already executed once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee9d373b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda!\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e188c2",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08a71b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"data/moby_dick.txt\"\n",
    "all_data = []\n",
    "with open(path_to_data, 'r', encoding=\"utf-8\") as f:\n",
    "    curr = []\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            curr.append(line)\n",
    "        else:\n",
    "            all_data.append(' '.join(curr))\n",
    "            curr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4a79ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "sentence_word = [word_tokenize(text.lower()) for text in all_data]\n",
    "sentence_word.sort(key=lambda x: len(x))\n",
    "sentence_word = [sentence for sentence in sentence_word if sentence]\n",
    "\n",
    "# Get maximum length of sentence\n",
    "max_seq_len = max(len(sentence) for sentence in sentence_word)\n",
    "print(max_seq_len)\n",
    "\n",
    "# Get smallest length of sentence\n",
    "print(min(len(sentence) for sentence in sentence_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d03ab33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['epilogue'], ['fore-top', '.'], ['sir', '?'], ['ahab', 'turned', '.'], ['chapter', '1.', 'loomings', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(sentence_word[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73e0dca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
    "curr = 4\n",
    "for sentence in sentence_word:\n",
    "    for word in sentence:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = curr\n",
    "            curr += 1\n",
    "\n",
    "idx2word = [''] * len(word2idx)\n",
    "for token, idx in word2idx.items():\n",
    "    idx2word[idx] = token\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    input = [word2idx['<SOS>']]\n",
    "    for word in sentence:\n",
    "        if word not in word2idx:\n",
    "            word = '<UNK>'\n",
    "        input.append(word2idx[word])\n",
    "\n",
    "    input.append(word2idx['<EOS>'])\n",
    "    output = input[1:] \n",
    "    return input, output\n",
    "\n",
    "def decode(sentence):\n",
    "    res = []\n",
    "    for idx in sentence:\n",
    "        res.append(idx2word[idx])\n",
    "    \n",
    "    return ' '.join(res)\n",
    "\n",
    "\n",
    "def add_padding(X, y, batch_size=32):\n",
    "    num_sentences = len(X)\n",
    "    pad_idx = word2idx['<PAD>'] \n",
    "    for i in range(0, num_sentences, batch_size):\n",
    "        idx = min(num_sentences-1, i + batch_size-1)\n",
    "        max_length = len(X[idx])\n",
    "        for j in range(i, idx+1):\n",
    "            missing_X = max_length - len(X[j])\n",
    "            \n",
    "            X[j].extend([pad_idx]*missing_X)\n",
    "            y[j].extend([pad_idx]*(missing_X + 1))\n",
    "\n",
    "    return X, y\n",
    "\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.long), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "batch_size = 16\n",
    "data = [process_sentence(sentence) for sentence in sentence_word]\n",
    "X = [sentence[0] for sentence in data]\n",
    "y = [sentence[1] for sentence in data]\n",
    "\n",
    "X_pad, y_pad = add_padding(X, y, batch_size)\n",
    "dataset = LMDataset(X_pad, y_pad)\n",
    "dataloader = DataLoader(dataset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61438816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 19625\n",
      "torch.Size([16, 6])\n",
      "torch.Size([16, 6])\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(word2idx)\n",
    "print(f\"Size of vocabulary: {len(word2idx)}\")\n",
    "\n",
    "for X, y in dataloader:\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b8b2fd",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "39ae39a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAttentionHead(nn.Module):\n",
    "    def __init__(self, num_heads, hidden_size):\n",
    "        super().__init__()\n",
    "        head_dim = hidden_size // num_heads\n",
    "\n",
    "        # Weight matrices for queries, keys, values\n",
    "        self.W_q = nn.Parameter(torch.randn(num_heads, hidden_size, head_dim))\n",
    "        self.W_k = nn.Parameter(torch.randn(num_heads, hidden_size, head_dim))\n",
    "        self.W_v = nn.Parameter(torch.randn(num_heads, hidden_size, head_dim))\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.head_dim = head_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Store the key and values\n",
    "        self.past_key = None \n",
    "        self.past_value = None \n",
    "\n",
    "        # Output linear projection\n",
    "        self.output_proj = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
    "\n",
    "        # Normalization layer\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):        \n",
    "        std = 0.02  \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.W_q.normal_(0, std)\n",
    "            self.W_k.normal_(0, std) \n",
    "            self.W_v.normal_(0, std)\n",
    "            self.output_proj.normal_(0, std)\n",
    "\n",
    "    def forward(self, X, padding_mask, auto_reg=False):\n",
    "        \"\"\"\n",
    "        Forward pass for multi-head attention.\n",
    "\n",
    "        Inputs:\n",
    "            X            : (batch_size, seq_len, hidden_size)\n",
    "            padding_mask : (batch_size, seq_len)  -> 1 for real tokens, 0 for padding\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = X.shape\n",
    "        device = X.device \n",
    "        X_heads = X.unsqueeze(1)\n",
    "\n",
    "        if not auto_reg:\n",
    "            # Create padding mask\n",
    "            mask_matrix = padding_mask.unsqueeze(-1) * padding_mask.unsqueeze(-2)\n",
    "            pad_mask = mask_matrix.unsqueeze(1).expand(-1, self.num_heads, -1, -1)\n",
    "\n",
    "            # Create causal mask\n",
    "            causal_mask = torch.tril(torch.ones((seq_len, seq_len), device=device))\n",
    "            causal_mask = causal_mask.unsqueeze(0).unsqueeze(0).expand(batch_size, self.num_heads, -1, -1)\n",
    "\n",
    "            # Combine masks\n",
    "            att_mask = pad_mask * causal_mask\n",
    "\n",
    "            # Add head dimension to input\n",
    "            X_heads = X.unsqueeze(1)  # (batch, 1, seq_len, hidden_size)\n",
    "\n",
    "            # Compute Q, K, V for full sequence\n",
    "            Q = torch.matmul(X_heads, self.W_q)  # (batch, num_heads, seq_len, head_dim)\n",
    "            K = torch.matmul(X_heads, self.W_k)  # (batch, num_heads, seq_len, head_dim)\n",
    "            V = torch.matmul(X_heads, self.W_v)  # (batch, num_heads, seq_len, head_dim)\n",
    "            \n",
    "        else:\n",
    "    \n",
    "            # Compute Q for current token only\n",
    "            Q = torch.matmul(X_heads, self.W_q)  # (batch, num_heads, 1, head_dim)\n",
    "            \n",
    "            if self.past_key is None:\n",
    "                # First token - compute K,V for the entire sequence so far\n",
    "                K = torch.matmul(X_heads, self.W_k)\n",
    "                V = torch.matmul(X_heads, self.W_v)\n",
    "            \n",
    "            else:\n",
    "                # Subsequent tokens - compute K,V only for current token and concatenate\n",
    "                curr_K = torch.matmul(X_heads, self.W_k)  # (batch, num_heads, 1, head_dim)\n",
    "                curr_V = torch.matmul(X_heads, self.W_v)\n",
    "                \n",
    "                # Concatenate with cached values\n",
    "                K = torch.cat([self.past_key, curr_K], dim=2)\n",
    "                V = torch.cat([self.past_value, curr_V], dim=2)\n",
    "\n",
    "            self.past_key = K\n",
    "            self.past_value = V\n",
    "\n",
    "\n",
    "        # Compute attention scores\n",
    "        K_transposed = K.transpose(-2, -1)  # (batch, num_heads, head_dim, kv_seq_len)\n",
    "        scores = torch.matmul(Q, K_transposed)  # (batch, num_heads, q_seq_len, kv_seq_len)\n",
    "        scaled_scores = scores / (self.head_dim ** 0.5)\n",
    "\n",
    "        # Apply mask and softmax\n",
    "        LARGE_NEG = -1e9\n",
    "        if not auto_reg:\n",
    "            scaled_scores = scaled_scores.masked_fill(~(att_mask == 1), LARGE_NEG)\n",
    "        \n",
    "        att_weights = torch.softmax(scaled_scores, dim=-1)\n",
    "\n",
    "        # Attention output\n",
    "        att_output = torch.matmul(att_weights, V)  # (batch, num_heads, q_seq_len, head_dim)\n",
    "        \n",
    "        if auto_reg:\n",
    "            combined_heads = att_output.reshape(batch_size, 1, self.hidden_size)\n",
    "        else:\n",
    "            combined_heads = att_output.reshape(batch_size, seq_len, self.hidden_size)\n",
    "\n",
    "        # Apply layer normalization and the final projection\n",
    "        norm_output = self.norm(combined_heads)\n",
    "        projected_output = torch.matmul(norm_output, self.output_proj)\n",
    "\n",
    "        return projected_output\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        self.past_key, self.past_value = None, None\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, hidden_layers=2, act='relu', hidden_size=[32, 32], input_size=32):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(input_size, hidden_size[0])]\n",
    "\n",
    "        for i in range(1, hidden_layers):\n",
    "            if act == 'relu':\n",
    "                layers.append(nn.ReLU())\n",
    "\n",
    "            elif act == 'tanh':\n",
    "                layers.append(nn.Tanh())\n",
    "            layers.append(nn.Linear(hidden_size[i-1], hidden_size[i]))\n",
    "            \n",
    "                \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_seq_len, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "        position = torch.arange(max_seq_len).unsqueeze(1) # (seq_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2) * (-math.log(10000.0) / embed_size))\n",
    "        pe = torch.zeros(1, max_seq_len, embed_size)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term) # (1, max_seq_len, embed_size)\n",
    "        self.register_buffer('pe', pe) # So it goes to CPU/GPU as well\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            X (batch_size, seq_len, embed_size)\n",
    "        \"\"\"\n",
    "        X = X + self.pe[:, :X.shape[1],:]\n",
    "        return self.dropout(X)  \n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, voc_size, embed_size, num_heads, depth, pad_idx, p=0.3):\n",
    "        super().__init__()\n",
    "        self.voc_size = voc_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = depth\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        # Embeddings layer\n",
    "        self.embed = nn.Embedding(voc_size, embed_size)\n",
    "        nn.init.xavier_uniform_(self.embed.weight)\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.pe = PositionalEncoding(embed_size=self.embed_size, max_seq_len=1000).to(device)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "        # Attention Heads\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                MultiAttentionHead(num_heads, embed_size),\n",
    "                nn.LayerNorm(embed_size),\n",
    "                FFN(hidden_layers=2, act=\"relu\",\n",
    "                    hidden_size=[4 * embed_size, embed_size],\n",
    "                    input_size=embed_size),\n",
    "                nn.LayerNorm(embed_size)\n",
    "            ])\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        # Last linear layer\n",
    "        self.linear = nn.Linear(embed_size, voc_size, bias=False)\n",
    "    \n",
    "\n",
    "    def forward(self, X, auto_reg=False):\n",
    "        '''\n",
    "        Inputs:\n",
    "            X (batch_size, seq_length)\n",
    "        '''\n",
    "        _, seq_len = X.shape\n",
    "        padding_mask = X != self.pad_idx\n",
    "\n",
    "        # Positional Encoding\n",
    "\n",
    "        X = self.embed(X) # (batch_size, seq_length, embedding_size)\n",
    "        X = self.pe(X)\n",
    "\n",
    "        if auto_reg:\n",
    "            X = X[:, -1:, :]  # (batch_size, 1, embed_size)\n",
    "\n",
    "        for head, norm1, ffn, norm2 in self.heads:\n",
    "            # Go through the multi heads\n",
    "            X = self.dropout(norm1(head(X, padding_mask, auto_reg) + X))\n",
    "\n",
    "            # Do the same for the FFN\n",
    "            X = self.dropout(norm2(ffn(X) + X))\n",
    "        \n",
    "        output = self.linear(X)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def generate_from_prompt(self, prompt, sample='k', max_size=1000):\n",
    "        self.eval() # We aren't training anymore\n",
    "        self.clear_memo() # Clear hashes\n",
    "        inputs, _ = process_sentence(prompt)\n",
    "        inputs = inputs[1:-1] # Remove <SOS> and <EOS>\n",
    "        curr = len(inputs)\n",
    "        while curr < max_size:\n",
    "            print(curr)\n",
    "            X = torch.tensor(inputs, dtype=torch.long).unsqueeze(0).to(device) # Add batch size 1\n",
    "            with torch.no_grad():\n",
    "                logits = self(X, auto_reg=True)\n",
    "            \n",
    "            # Sample from logits\n",
    "            if sample == 'k':\n",
    "                token = self.top_k_sample(logits)[0, 0].item()\n",
    "                \n",
    "            elif sample == 'greedy':\n",
    "                token = self.greedy_sample(logits)[0, 0].item()\n",
    "            \n",
    "            # See if end of sentence\n",
    "            if idx2word[token] == '<EOS>':\n",
    "                break\n",
    "            \n",
    "            curr += 1\n",
    "            inputs.append(token)\n",
    "        \n",
    "        return decode(inputs)\n",
    "\n",
    "    def clear_memo(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, MultiAttentionHead):\n",
    "                module.clear_cache()\n",
    "\n",
    "\n",
    "    def greedy_sample(self, output):\n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(output, dim=-1)\n",
    "\n",
    "\n",
    "    def top_k_sample(self, output, k=5):\n",
    "        '''\n",
    "        Inputs:\n",
    "            output (batch_size, seq_len, voc_size)\n",
    "        '''\n",
    "        batch_size, seq_len, _ = output.shape \n",
    "        res = torch.zeros((batch_size, seq_len), dtype=torch.long).to(device)\n",
    "        with torch.no_grad():\n",
    "            probabilities = torch.softmax(output, dim=-1)\n",
    "            values, indices = torch.topk(probabilities, k=k, dim=-1)\n",
    "            batch_idx = torch.arange(batch_size)\n",
    "            for pos in range(seq_len): \n",
    "                sample = torch.multinomial(values[:, pos, :], num_samples=1).squeeze(1) # (batch_size)\n",
    "                res[:, pos] = indices[batch_idx, pos, sample]\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "107fc4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: torch.Size([2, 5, 512])\n",
      "torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "# Example dimensions\n",
    "batch_size = 2    # number of sequences\n",
    "seq_len = 5       # tokens per sequence\n",
    "hidden_size = 512   # embedding dimension\n",
    "h = 8 # number of heads\n",
    "head = MultiAttentionHead(h, hidden_size).to(device)\n",
    "\n",
    "# Create random input\n",
    "X = torch.randn(batch_size, seq_len, hidden_size).to(device)\n",
    "att_mask = (torch.randint(0, 2, (batch_size, seq_len)) == 1).to(device)\n",
    "output = head(X, att_mask)\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "81777d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'some years boys half-articulated beaching collar beaching boys beaching beaching transfixedly beaching boys transfixedly awed homeward homeward homeward boys homeward transfixedly awed calm—frozen homeward rearward rearward rearward transfixedly transfixedly beaching homeward transfixedly beaching transfixedly transfixedly homeward calm—frozen aboriginal awed unprejudiced awed awed evenly awed awed aboriginal unprejudiced transfixedly awed unprejudiced evenly evenly tore awed transfixedly homeward awed unprejudiced awed boys awed awed evenly transfixedly evenly awed awed evenly homeward evenly unprejudiced evenly tore strands tore and and tore rover rover homeward calm—frozen homeward and and homeward transfixedly transfixedly aboriginal unprejudiced and and transfixedly transfixedly evenly evenly transfixedly evenly homeward unprejudiced evenly transfixedly tore evenly tore unprejudiced rearward turns transfixedly evenly transfixedly unprejudiced evenly tore unprejudiced wine tore homeward homeward tore transfixedly wine evenly evenly transfixedly unprejudiced rover tore rover homeward tore transfixedly evenly evenly tore wine wine evenly tore strands evenly tore rover transfixedly tore transfixedly evenly transfixedly evenly tore tore transfixedly rover evenly transfixedly homeward transfixedly tore tore tore rearward tore turns cut turns cut homeward homeward cut transfixedly cut evenly cut evenly transfixedly transfixedly cut rearward evenly transfixedly unprejudiced evenly homeward unprejudiced evenly evenly cut homeward evenly evenly transfixedly evenly evenly unprejudiced transfixedly transfixedly cut homeward homeward cut evenly wine cut silk transfixedly cut homeward unprejudiced homeward cut transfixedly evenly homeward evenly cut cut and evenly cut evenly rearward transfixedly cut unprejudiced unprejudiced transfixedly evenly cut cut cut rearward cut homeward homeward rearward cut cut homeward cut transfixedly evenly unprejudiced cut cut transfixedly transfixedly homeward cut unprejudiced tore homeward transfixedly cut unprejudiced tore cut cut evenly unprejudiced cut unprejudiced aboriginal cut cut cut evenly transfixedly paracelsan unprejudiced unprejudiced transfixedly unprejudiced unprejudiced unprejudiced unprejudiced transfixedly unprejudiced cut cut cut transfixedly cut cut unprejudiced cut tree transfixedly cut homeward transfixedly unprejudiced transfixedly unprejudiced cut rearward cut affecting unprejudiced tree unprejudiced cut unprejudiced tore tore transfixedly unprejudiced unprejudiced unprejudiced cut unprejudiced evenly unprejudiced cut homeward cut cut cut affecting 30. unprejudiced 30. cut homeward transfixedly cut cut rover cut cut steed affecting cut cut transfixedly cut transfixedly cut cut cut affecting 30. cut homeward transfixedly cut cut evenly cut transfixedly cut cut cut affecting transfixedly cut homeward cut affecting cut affecting transfixedly cut cut affecting cut homeward evenly transfixedly affecting cut cut affecting transfixedly cut transfixedly cut affecting evenly transfixedly transfixedly homeward homeward transfixedly tore tore cut homeward affecting cut affecting unprejudiced evenly cut homeward unprejudiced affecting affecting tore affecting unprejudiced transfixedly homeward unprejudiced transfixedly rover tore cut transfixedly tore tore cut cut cut rover cut tore cut cut homeward transfixedly cut cut tore transfixedly homeward transfixedly homeward cut steed cut rearward cut paracelsan transfixedly transfixedly unprejudiced homeward homeward transfixedly homeward affecting transfixedly cut tore tore transfixedly tore transfixedly secludedness cut secludedness transfixedly schoolmasters transfixedly ripples cut transfixedly secludedness homeward homeward transfixedly affecting rover transfixedly affecting homeward transfixedly transfixedly cut affecting tore cut affecting affecting secludedness secludedness secludedness tore secludedness affecting cut cut transfixedly affecting secludedness secludedness secludedness transfixedly affecting secludedness secludedness secludedness transfixedly affecting homeward transfixedly cut transfixedly transfixedly constantinople cut cut transfixedly homeward secludedness secludedness secludedness cut secludedness transfixedly affecting cut transfixedly transfixedly cut rover affecting cut tore cut paracelsan secludedness transfixedly affecting secludedness secludedness affecting transfixedly transfixedly transfixedly cut transfixedly homeward transfixedly transfixedly cut affecting secludedness cut affecting homeward cut cut transfixedly secludedness serenity homeward homeward transfixedly affecting serenity steed cut paracelsan paracelsan cut affecting paracelsan affecting paracelsan paracelsan secludedness paracelsan affecting paracelsan transfixedly homeward homeward paracelsan paracelsan affecting secludedness affecting homeward homeward paracelsan homeward affecting homeward affecting affecting secludedness affecting affecting transfixedly cut affecting cut transfixedly homeward homeward schoolmasters cut cut cut cut homeward cut affecting aboriginal affecting affecting transfixedly aboriginal homeward cut cut constantinople cut evenly affecting aboriginal schoolmasters transfixedly homeward cut cut cut cut transfixedly transfixedly cut affecting affecting evenly evenly affecting affecting aboriginal cut affecting affecting affecting affecting transfixedly paracelsan aboriginal aboriginal paracelsan cut cut aboriginal homeward transfixedly cut cut cut paracelsan cut transfixedly paracelsan secludedness secludedness aboriginal transfixedly transfixedly cut transfixedly paracelsan paracelsan cut homeward affecting affecting cut paracelsan paracelsan paracelsan evenly affecting evenly secludedness cut strands paracelsan cut homeward cut secludedness paracelsan transfixedly paracelsan secludedness paracelsan affecting secludedness cut secludedness secludedness secludedness cut transfixedly cut secludedness secludedness secludedness cut affecting secludedness transfixedly cut paracelsan secludedness paracelsan cut affecting cut homeward cut affecting affecting affecting provincialisms provincialisms affecting provincialisms transfixedly cut transfixedly serenity affecting affecting aboriginal aboriginal aboriginal transfixedly paracelsan secludedness paracelsan secludedness paracelsan affecting homeward homeward homeward paracelsan affecting provincialisms cut homeward affecting transfixedly transfixedly cut affecting provincialisms cut 30. transfixedly cut transfixedly aboriginal provincialisms provincialisms cut tore tore homeward homeward cut tore tore paracelsan secludedness paracelsan paracelsan affecting transfixedly secludedness transfixedly paracelsan tore homeward homeward secludedness transfixedly transfixedly serenity homeward transfixedly tore homeward transfixedly tore transfixedly homeward paracelsan paracelsan tore transfixedly homeward transfixedly cut secludedness secludedness transfixedly secludedness transfixedly secludedness homeward transfixedly cut cut cut homeward homeward homeward affecting transfixedly affecting aboriginal homeward affecting transfixedly homeward transfixedly affecting transfixedly affecting affecting homeward aboriginal homeward affecting transfixedly tore affecting transfixedly homeward cut cut affecting affecting affecting cut transfixedly tore cut cut affecting transfixedly affecting transfixedly cut transfixedly transfixedly transfixedly rover homeward rover cut cut look-e cut affecting cut homeward affecting transfixedly aboriginal transfixedly affecting look-e look-e transfixedly homeward affecting affecting homeward homeward homeward transfixedly affecting homeward transfixedly paracelsan cut affecting affecting aboriginal aboriginal tore paracelsan homeward look-e homeward homeward homeward affecting affecting tore homeward tore tore transfixedly look-e tore transfixedly affecting paracelsan affecting affecting homeward homeward provincialisms affecting homeward constantinople 30. affecting paracelsan homeward affecting affecting transfixedly constantinople constantinople tore homeward homeward cut paracelsan affecting paracelsan affecting affecting rover rover rover paracelsan rover paracelsan transfixedly paracelsan homeward homeward transfixedly paracelsan homeward affecting paracelsan constantinople constantinople constantinople paracelsan transfixedly paracelsan constantinople affecting affecting transfixedly constantinople transfixedly transfixedly transfixedly transfixedly evenly transfixedly cut constantinople constantinople cut homeward affecting cut affecting cut homeward transfixedly transfixedly secludedness affecting cut affecting cut cut paracelsan homeward transfixedly cut homeward look-e affecting transfixedly transfixedly cut cut paracelsan paracelsan cut affecting cut homeward cut constantinople transfixedly homeward aboriginal affecting affecting'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = Transformer(voc_size=len(word2idx), embed_size=512, num_heads=8, depth=12,\\\n",
    "                          pad_idx=word2idx['<PAD>'], p=0.3).to(device)\n",
    "\n",
    "prompt = \"Some years\"\n",
    "transformer.generate_from_prompt(prompt.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2ca8946e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape of k sampling: torch.Size([10, 6])\n",
      "Output shape of greedy sampling: torch.Size([10, 6])\n"
     ]
    }
   ],
   "source": [
    "X_test = torch.tensor(X_pad[:10]).to(device)\n",
    "transformer = Transformer(voc_size=len(word2idx), embed_size=512, num_heads=8, depth=12,\\\n",
    "                          pad_idx=word2idx['<PAD>'], p=0.3).to(device)\n",
    "\n",
    "output = transformer(X_test)\n",
    "\n",
    "prediction_k = transformer.top_k_sample(output)\n",
    "prediction_greedy = transformer.greedy_sample(output)\n",
    "print(f\"Output shape of k sampling: {prediction_k.shape}\")\n",
    "print(f\"Output shape of greedy sampling: {prediction_greedy.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "2353d19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text for k sampling:\n",
      "escaped biased unusual evanescent melodious struggle\n",
      "selecting unthinking oftentimes congeniality outwardly charter\n",
      "mum repentant vagueness highway sagacious tender\n",
      "hopped afflicted worthy cents small-e elevated\n",
      "true—ye palate excitedly wherever undefiled tested\n",
      "morrel aggrieved—this border lying-in it conjure\n",
      "ancestry speaks cultured gouty viewed pomatum\n",
      "gallop sitting organized thrasher threads beaver\n",
      "treacherous wolfish tails mishap undashed death\n",
      "lawless creamy suckling —very stall-fed clouds\n",
      "\n",
      "Text for greedy sampling:\n",
      "speculative precincts content fadest turtles struggle\n",
      "mind—it unthinking waves—the motioned flat-faced kindness\n",
      "torso upright oxygenated highway sagacious now—was\n",
      "me afflicted rise—yes cents small-e water-line\n",
      "sunwards anacharsis prick clifford united hatches—don\n",
      "averse aggrieved—this nate reveries—stand whiteness—though conjure\n",
      "ancestry skylarking comforts escape viewed propped\n",
      "blasted mortar descry formation grapes beaver\n",
      "anchor—as lint comparable codfish enfoldings death\n",
      "insurances to—justice ripening equalled rushing bridegroom\n"
     ]
    }
   ],
   "source": [
    "print(\"Text for k sampling:\")\n",
    "for batch in prediction_k:\n",
    "    print(decode(batch))\n",
    "\n",
    "print(\"\\nText for greedy sampling:\")\n",
    "\n",
    "for batch in prediction_greedy:\n",
    "    print(decode(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "5047b869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, optimizer, scheduler, pad_idx):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "    total_loss = 0.0\n",
    "    for i, (batch_X, batch_y) in enumerate(train_loader):\n",
    "        if i % 30 == 0 or i == len(train_loader) - 1:\n",
    "            print(f\"{(i / len(train_loader)) * 100:.2f}% done\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device) # (batch_size, seq_len)\n",
    "        \n",
    "        outputs = model(batch_X) # (batch_size, seq_len, voc_size)\n",
    "        outputs = outputs.permute(0, 2, 1) # (batch_size, voc_size, seq_len) -> expected size for nn.CrossEntropy\n",
    "\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() \n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    return total_loss / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "816f7984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_transformer_training(model, total_steps, warmup_steps):\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=5e-4,             \n",
    "        weight_decay=0.01   \n",
    "    )\n",
    "\n",
    "    # Learning rate schedule with warmup + linear decay to zero\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lr_lambda=lambda step: min((step + 1) / warmup_steps, \n",
    "                                   max(0.0, (total_steps - step) / max(1, total_steps - warmup_steps)))\n",
    "    )\n",
    "\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a4e1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "0.00% done\n",
      "18.63% done\n",
      "37.27% done\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "warmup_epochs = 10\n",
    "pad_idx = word2idx['<PAD>']\n",
    "model = Transformer(voc_size=len(word2idx), embed_size=512, num_heads=8, depth=12,\\\n",
    "                    pad_idx=word2idx['<PAD>'], p=0.3).to(device)\n",
    "opt, scheduler = setup_transformer_training(model, num_epochs, warmup_epochs)\n",
    "\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    loss = train_one_epoch(model, dataloader, opt, scheduler, pad_idx)\n",
    "    print(f\"Loss: {loss}\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        print(f\"Found better model at epoch {epoch}. Saving...\")\n",
    "        torch.save(model.state_dict(), \"models/best_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

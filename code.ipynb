{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ae530d5",
   "metadata": {},
   "source": [
    "# Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d210ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b656171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt_tab') -> Already executed once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee9d373b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu!\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e188c2",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08a71b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"data/moby_dick.txt\"\n",
    "all_data = []\n",
    "with open(path_to_data, 'r', encoding=\"utf-8\") as f:\n",
    "    curr = []\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            curr.append(line)\n",
    "        else:\n",
    "            all_data.append(' '.join(curr))\n",
    "            curr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4a79ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "sentence_word = [word_tokenize(text.lower()) for text in all_data]\n",
    "sentence_word.sort(key=lambda x: len(x))\n",
    "sentence_word = [sentence for sentence in sentence_word if sentence]\n",
    "\n",
    "# Get maximum length of sentence\n",
    "max_seq_len = max(len(sentence) for sentence in sentence_word)\n",
    "print(max_seq_len)\n",
    "\n",
    "# Get smallest length of sentence\n",
    "print(min(len(sentence) for sentence in sentence_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d03ab33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['epilogue'], ['fore-top', '.'], ['sir', '?'], ['ahab', 'turned', '.'], ['chapter', '1.', 'loomings', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(sentence_word[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73e0dca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
    "curr = 4\n",
    "for sentence in sentence_word:\n",
    "    for word in sentence:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = curr\n",
    "            curr += 1\n",
    "\n",
    "idx2word = [''] * len(word2idx)\n",
    "for token, idx in word2idx.items():\n",
    "    idx2word[idx] = token\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    input = [word2idx['<SOS>']]\n",
    "    for word in sentence:\n",
    "        if word not in word2idx:\n",
    "            word = '<UNK>'\n",
    "        input.append(word2idx[word])\n",
    "\n",
    "    input.append(word2idx['<EOS>'])\n",
    "    output = input[1:] \n",
    "    return input, output\n",
    "\n",
    "def decode(sentence):\n",
    "    res = []\n",
    "    for idx in sentence:\n",
    "        res.append(idx2word[idx])\n",
    "    \n",
    "    return ' '.join(res)\n",
    "\n",
    "\n",
    "def add_padding(X, y, batch_size=32):\n",
    "    num_sentences = len(X)\n",
    "    pad_idx = word2idx['<PAD>'] \n",
    "    for i in range(0, num_sentences, batch_size):\n",
    "        idx = min(num_sentences-1, i + batch_size-1)\n",
    "        max_length = len(X[idx])\n",
    "        for j in range(i, idx+1):\n",
    "            missing_X = max_length - len(X[j])\n",
    "            \n",
    "            X[j].extend([pad_idx]*missing_X)\n",
    "            y[j].extend([pad_idx]*(missing_X + 1))\n",
    "\n",
    "    return X, y\n",
    "\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.long), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "batch_size = 32\n",
    "data = [process_sentence(sentence) for sentence in sentence_word]\n",
    "X = [sentence[0] for sentence in data]\n",
    "y = [sentence[1] for sentence in data]\n",
    "\n",
    "X_pad, y_pad = add_padding(X, y, batch_size)\n",
    "dataset = LMDataset(X_pad, y_pad)\n",
    "dataloader = DataLoader(dataset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61438816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 19625\n",
      "torch.Size([32, 6])\n",
      "torch.Size([32, 6])\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(word2idx)\n",
    "print(f\"Size of vocabulary: {len(word2idx)}\")\n",
    "\n",
    "for X, y in dataloader:\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b8b2fd",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39ae39a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAttentionHead(nn.Module):\n",
    "    def __init__(self, num_heads, hidden_size):\n",
    "        super().__init__()\n",
    "        head_dim = hidden_size // num_heads\n",
    "\n",
    "        # Weight matrices for queries, keys, values\n",
    "        self.W_q = nn.Parameter(torch.randn(num_heads, hidden_size, head_dim))\n",
    "        self.W_k = nn.Parameter(torch.randn(num_heads, hidden_size, head_dim))\n",
    "        self.W_v = nn.Parameter(torch.randn(num_heads, hidden_size, head_dim))\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.head_dim = head_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Output linear projection\n",
    "        self.output_proj = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
    "\n",
    "        # Normalization layer\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):        \n",
    "        std = 0.02  \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.W_q.normal_(0, std)\n",
    "            self.W_k.normal_(0, std) \n",
    "            self.W_v.normal_(0, std)\n",
    "            self.output_proj.normal_(0, std)\n",
    "\n",
    "    def forward(self, X, padding_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for multi-head attention.\n",
    "\n",
    "        Inputs:\n",
    "            X            : (batch_size, seq_len, hidden_size)\n",
    "            padding_mask : (batch_size, seq_len)  -> 1 for real tokens, 0 for padding\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = X.shape\n",
    "\n",
    "        # Create padding mask\n",
    "        mask_matrix = padding_mask.unsqueeze(-1) * padding_mask.unsqueeze(-2)  # (batch, seq_len, seq_len)\n",
    "        pad_mask = mask_matrix.unsqueeze(1).expand(-1, self.num_heads, -1, -1)\n",
    "\n",
    "        # Create causal mask\n",
    "        causal_mask = torch.tril(torch.ones((seq_len, seq_len), device=device)).unsqueeze(0).expand(batch_size, self.num_heads, -1, -1)\n",
    "\n",
    "        # Finally, create attention mask\n",
    "        att_mask = pad_mask * causal_mask\n",
    "\n",
    "        # Add head dimension to input\n",
    "        X_heads = X.unsqueeze(1)  # (batch, 1, seq_len, hidden_size)\n",
    "\n",
    "        # Compute Q, K, V\n",
    "        Q, K, V = torch.matmul(X_heads, self.W_q), torch.matmul(X_heads, self.W_k), \\\n",
    "                  torch.matmul(X_heads, self.W_v)  \n",
    "\n",
    "        # Compute attention scores\n",
    "        K_transposed = K.transpose(-2, -1)\n",
    "        scores = torch.matmul(Q, K_transposed)\n",
    "        scaled_scores = scores / (self.head_dim ** 0.5)\n",
    "\n",
    "        # Apply mask and softmax\n",
    "        LARGE_NEG = -1e9\n",
    "        masked_scores = scaled_scores.masked_fill(~(att_mask == 1), LARGE_NEG)\n",
    "        att_weights = torch.softmax(masked_scores, dim=-1)\n",
    "\n",
    "        # Attention output\n",
    "        att_output = torch.matmul(att_weights, V) # (batch, num_heads, seq_len, head_dim)\n",
    "        combined_heads = att_output.reshape(batch_size, seq_len, self.hidden_size)\n",
    "\n",
    "        # Apply layer normalization and the final projection\n",
    "        norm_output = self.norm(combined_heads)\n",
    "        projected_output = torch.matmul(norm_output, self.output_proj)\n",
    "\n",
    "        return projected_output # (batch_size, seq_length, embed_size)\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, hidden_layers=2, act='relu', hidden_size=[32, 32], input_size=32):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(input_size, hidden_size[0])]\n",
    "\n",
    "        for i in range(1, hidden_layers):\n",
    "            if act == 'relu':\n",
    "                layers.append(nn.ReLU())\n",
    "\n",
    "            elif act == 'tanh':\n",
    "                layers.append(nn.Tanh())\n",
    "            layers.append(nn.Linear(hidden_size[i-1], hidden_size[i]))\n",
    "            \n",
    "                \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_seq_len, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "        position = torch.arange(max_seq_len).unsqueeze(1) # (seq_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2) * (-math.log(10000.0) / embed_size))\n",
    "        pe = torch.zeros(1, max_seq_len, embed_size)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term) # (1, max_seq_len, embed_size)\n",
    "        self.register_buffer('pe', pe) # So it goes to CPU/GPU as well\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            X (batch_size, seq_len, embed_size)\n",
    "        \"\"\"\n",
    "        X = X + self.pe[:, X.shape[1],:]\n",
    "        return self.dropout(X)  \n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, voc_size, embed_size, num_heads, depth, pad_idx, max_seq_len, p=0.3):\n",
    "        super().__init__()\n",
    "        self.voc_size = voc_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = depth\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        # Embeddings layer\n",
    "        self.embed = nn.Embedding(voc_size, embed_size)\n",
    "        nn.init.xavier_uniform_(self.embed.weight)\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.pe = PositionalEncoding(embed_size=self.embed_size, max_seq_len=max_seq_len).to(device)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "        # Attention Heads\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                MultiAttentionHead(num_heads, embed_size),\n",
    "                nn.LayerNorm(embed_size),\n",
    "                FFN(hidden_layers=2, act=\"relu\",\n",
    "                    hidden_size=[4 * embed_size, embed_size],\n",
    "                    input_size=embed_size),\n",
    "                nn.LayerNorm(embed_size)\n",
    "            ])\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        # Last linear layer\n",
    "        self.linear = nn.Linear(embed_size, voc_size, bias=False)\n",
    "    \n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        Inputs:\n",
    "            X (batch_size, seq_length)\n",
    "        '''\n",
    "        _, seq_len = X.shape\n",
    "        padding_mask = X != self.pad_idx\n",
    "\n",
    "        # Positional Encoding\n",
    "        \n",
    "\n",
    "        X = self.embed(X) # (batch_size, seq_length, embedding_size)\n",
    "        X = self.pe(X)\n",
    "\n",
    "        for head, norm1, ffn, norm2 in self.heads:\n",
    "            # Go through the multi heads\n",
    "            X = self.dropout(norm1(head(X, padding_mask) + X))\n",
    "\n",
    "            # Do the same for the FFN\n",
    "            X = self.dropout(norm2(ffn(X) + X))\n",
    "        \n",
    "        output = self.linear(X)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def predict(self, output):\n",
    "        return torch.argmax(output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "107fc4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: torch.Size([2, 5, 512])\n",
      "torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "# Example dimensions\n",
    "batch_size = 2    # number of sequences\n",
    "seq_len = 5       # tokens per sequence\n",
    "hidden_size = 512   # embedding dimension\n",
    "h = 8 # number of heads\n",
    "head = MultiAttentionHead(h, hidden_size)\n",
    "\n",
    "# Create random input\n",
    "X = torch.randn(batch_size, seq_len, hidden_size)\n",
    "att_mask = torch.randint(0, 2, (batch_size, seq_len)) == 1\n",
    "output = head(X, att_mask)\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ca8946e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([10, 6])\n"
     ]
    }
   ],
   "source": [
    "X_test = torch.tensor(X_pad[:10])\n",
    "transformer = Transformer(voc_size=len(word2idx), embed_size=512, num_heads=8, depth=12,\\\n",
    "                          pad_idx=word2idx['<PAD>'], max_seq_len=max_seq_len, p=0.3)\n",
    "\n",
    "output = transformer(X_test)\n",
    "\n",
    "prediction = transformer.predict(output)\n",
    "print(f\"Output shape: {prediction.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2353d19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "livelihood trample rub poorest queequeg—especially beard\n",
      "blasphemy space following fetch affairs synod\n",
      "effectual fleecy thwack mythologies propulsion sleepiest\n",
      "gallied please going woebegone ezekiel onsets\n",
      "collectively purporting domineer heroic bird—airley encamp\n",
      "contemplating coffer-dam swords reverie corresponding enemies\n",
      "moody agent cough oftentimes bawling naturalist\n",
      "himself—these warn coal-black prevent bread—but nuptial\n",
      "victory beaches pregnant hairs freebooters mesopotamian\n",
      "epitaphs horse-pieces gout professional _how_ available\n"
     ]
    }
   ],
   "source": [
    "for batch in prediction:\n",
    "    print(decode(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5047b869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, optimizer, pad_idx):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "    total_loss = 0.0\n",
    "    for i, (batch_X, batch_y) in enumerate(train_loader):\n",
    "        if i % 30 == 0 or i == len(train_loader) - 1:\n",
    "            print(f\"{(i / len(train_loader)) * 100:.2f}% done\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device) # (batch_size, seq_len)\n",
    "        \n",
    "        outputs = model(batch_X) # (batch_size, seq_len, voc_size)\n",
    "        outputs = outputs.permute(0, 2, 1) # (batch_size, voc_size, seq_len) -> expected size for nn.CrossEntropy\n",
    "\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() \n",
    "        print(loss.item())\n",
    "\n",
    "\n",
    "    return total_loss / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a4e1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "0.00% done\n",
      "10.231156349182129\n",
      "8.078489303588867\n",
      "6.014791011810303\n",
      "6.286138534545898\n",
      "6.9852495193481445\n",
      "6.9184184074401855\n",
      "7.655981540679932\n",
      "7.61412239074707\n",
      "7.254820346832275\n",
      "6.670977592468262\n",
      "6.6685075759887695\n",
      "6.65748929977417\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs+\u001b[32m1\u001b[39m):\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, train_loader, optimizer, pad_idx)\u001b[39m\n\u001b[32m     15\u001b[39m outputs = outputs.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m) \u001b[38;5;66;03m# (batch_size, voc_size, seq_len) -> expected size for nn.CrossEntropy\u001b[39;00m\n\u001b[32m     17\u001b[39m loss = criterion(outputs, batch_y)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m0.5\u001b[39m)\n\u001b[32m     23\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pedri\\Desktop\\personal_project\\my_env\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pedri\\Desktop\\personal_project\\my_env\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pedri\\Desktop\\personal_project\\my_env\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "pad_idx = word2idx['<PAD>']\n",
    "model = Transformer(voc_size=len(word2idx), embed_size=512, num_heads=8, depth=12,\\\n",
    "                    pad_idx=word2idx['<PAD>'], max_seq_len=max_seq_len, p=0.3)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for i in range(1, num_epochs+1):\n",
    "    print(f\"Epoch {i}\")\n",
    "    loss = train_one_epoch(model, dataloader, opt, pad_idx)\n",
    "    print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247e330a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

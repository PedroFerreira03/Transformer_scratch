{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ae530d5",
   "metadata": {},
   "source": [
    "# Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d210ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b656171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt_tab') -> Already executed once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee9d373b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda!\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e188c2",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08a71b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"data/moby_dick.txt\"\n",
    "all_data = []\n",
    "with open(path_to_data, 'r', encoding=\"utf-8\") as f:\n",
    "    curr = []\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            curr.append(line)\n",
    "        else:\n",
    "            all_data.append(' '.join(curr))\n",
    "            curr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4a79ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "sentence_word = [word_tokenize(text.lower()) for text in all_data]\n",
    "sentence_word.sort(key=lambda x: len(x))\n",
    "sentence_word = [sentence for sentence in sentence_word if sentence]\n",
    "\n",
    "# Get maximum length of sentence\n",
    "max_seq_len = max(len(sentence) for sentence in sentence_word)\n",
    "print(max_seq_len)\n",
    "\n",
    "# Get smallest length of sentence\n",
    "print(min(len(sentence) for sentence in sentence_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d03ab33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['epilogue'], ['fore-top', '.'], ['sir', '?'], ['ahab', 'turned', '.'], ['chapter', '1.', 'loomings', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(sentence_word[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73e0dca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
    "curr = 4\n",
    "for sentence in sentence_word:\n",
    "    for word in sentence:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = curr\n",
    "            curr += 1\n",
    "\n",
    "idx2word = [''] * len(word2idx)\n",
    "for token, idx in word2idx.items():\n",
    "    idx2word[idx] = token\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    input = [word2idx['<SOS>']]\n",
    "    for word in sentence:\n",
    "        if word not in word2idx:\n",
    "            word = '<UNK>'\n",
    "        input.append(word2idx[word])\n",
    "\n",
    "    input.append(word2idx['<EOS>'])\n",
    "    output = input[1:] \n",
    "    return input, output\n",
    "\n",
    "def decode(sentence):\n",
    "    res = []\n",
    "    for idx in sentence:\n",
    "        res.append(idx2word[idx])\n",
    "    \n",
    "    return ' '.join(res)\n",
    "\n",
    "\n",
    "def add_padding(X, y, batch_size=32):\n",
    "    num_sentences = len(X)\n",
    "    pad_idx = word2idx['<PAD>'] \n",
    "    for i in range(0, num_sentences, batch_size):\n",
    "        idx = min(num_sentences-1, i + batch_size-1)\n",
    "        max_length = len(X[idx])\n",
    "        for j in range(i, idx+1):\n",
    "            missing_X = max_length - len(X[j])\n",
    "            \n",
    "            X[j].extend([pad_idx]*missing_X)\n",
    "            y[j].extend([pad_idx]*(missing_X + 1))\n",
    "\n",
    "    return X, y\n",
    "\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.long), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "batch_size = 16\n",
    "data = [process_sentence(sentence) for sentence in sentence_word]\n",
    "X = [sentence[0] for sentence in data]\n",
    "y = [sentence[1] for sentence in data]\n",
    "\n",
    "X_pad, y_pad = add_padding(X, y, batch_size)\n",
    "dataset = LMDataset(X_pad, y_pad)\n",
    "dataloader = DataLoader(dataset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61438816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 19625\n",
      "torch.Size([16, 6])\n",
      "torch.Size([16, 6])\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(word2idx)\n",
    "print(f\"Size of vocabulary: {len(word2idx)}\")\n",
    "\n",
    "for X, y in dataloader:\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b8b2fd",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ae39a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAttentionHead(nn.Module):\n",
    "    def __init__(self, num_heads, hidden_size):\n",
    "        super().__init__()\n",
    "        head_dim = hidden_size // num_heads\n",
    "\n",
    "        # Weight matrices for queries, keys, values\n",
    "        self.W_q = nn.Parameter(torch.randn(num_heads, hidden_size, head_dim))\n",
    "        self.W_k = nn.Parameter(torch.randn(num_heads, hidden_size, head_dim))\n",
    "        self.W_v = nn.Parameter(torch.randn(num_heads, hidden_size, head_dim))\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.head_dim = head_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Store the key and values\n",
    "        self.past_key = None \n",
    "        self.past_value = None \n",
    "\n",
    "        # Output linear projection\n",
    "        self.output_proj = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
    "\n",
    "        # Normalization layer\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):        \n",
    "        std = 0.02  \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.W_q.normal_(0, std)\n",
    "            self.W_k.normal_(0, std) \n",
    "            self.W_v.normal_(0, std)\n",
    "            self.output_proj.normal_(0, std)\n",
    "\n",
    "    def forward(self, X, padding_mask, auto_reg=False):\n",
    "        \"\"\"\n",
    "        Forward pass for multi-head attention.\n",
    "\n",
    "        Inputs:\n",
    "            X            : (batch_size, seq_len, hidden_size)\n",
    "            padding_mask : (batch_size, seq_len)  -> 1 for real tokens, 0 for padding\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = X.shape\n",
    "        device = X.device \n",
    "        X_heads = X.unsqueeze(1)\n",
    "\n",
    "        if not auto_reg:\n",
    "            # Create padding mask\n",
    "            mask_matrix = padding_mask.unsqueeze(-1) * padding_mask.unsqueeze(-2)\n",
    "            pad_mask = mask_matrix.unsqueeze(1).expand(-1, self.num_heads, -1, -1)\n",
    "\n",
    "            # Create causal mask\n",
    "            causal_mask = torch.tril(torch.ones((seq_len, seq_len), device=device))\n",
    "            causal_mask = causal_mask.unsqueeze(0).unsqueeze(0).expand(batch_size, self.num_heads, -1, -1)\n",
    "\n",
    "            # Combine masks\n",
    "            att_mask = pad_mask * causal_mask\n",
    "\n",
    "            # Add head dimension to input\n",
    "            X_heads = X.unsqueeze(1)  # (batch, 1, seq_len, hidden_size)\n",
    "\n",
    "            # Compute Q, K, V for full sequence\n",
    "            Q = torch.matmul(X_heads, self.W_q)  # (batch, num_heads, seq_len, head_dim)\n",
    "            K = torch.matmul(X_heads, self.W_k)  # (batch, num_heads, seq_len, head_dim)\n",
    "            V = torch.matmul(X_heads, self.W_v)  # (batch, num_heads, seq_len, head_dim)\n",
    "            \n",
    "        else:\n",
    "    \n",
    "            # Compute Q for current token only\n",
    "            Q = torch.matmul(X_heads, self.W_q)  # (batch, num_heads, 1, head_dim)\n",
    "            \n",
    "            if self.past_key is None:\n",
    "                # First token - compute K,V for the entire sequence so far\n",
    "                K = torch.matmul(X_heads, self.W_k)\n",
    "                V = torch.matmul(X_heads, self.W_v)\n",
    "            \n",
    "            else:\n",
    "                # Subsequent tokens - compute K,V only for current token and concatenate\n",
    "                curr_K = torch.matmul(X_heads, self.W_k)  # (batch, num_heads, 1, head_dim)\n",
    "                curr_V = torch.matmul(X_heads, self.W_v)\n",
    "                \n",
    "                # Concatenate with cached values\n",
    "                K = torch.cat([self.past_key, curr_K], dim=2)\n",
    "                V = torch.cat([self.past_value, curr_V], dim=2)\n",
    "\n",
    "            self.past_key = K\n",
    "            self.past_value = V\n",
    "\n",
    "\n",
    "        # Compute attention scores\n",
    "        K_transposed = K.transpose(-2, -1)  # (batch, num_heads, head_dim, kv_seq_len)\n",
    "        scores = torch.matmul(Q, K_transposed)  # (batch, num_heads, q_seq_len, kv_seq_len)\n",
    "        scaled_scores = scores / (self.head_dim ** 0.5)\n",
    "\n",
    "        # Apply mask and softmax\n",
    "        LARGE_NEG = -1e9\n",
    "        if not auto_reg:\n",
    "            scaled_scores = scaled_scores.masked_fill(~(att_mask == 1), LARGE_NEG)\n",
    "        \n",
    "        att_weights = torch.softmax(scaled_scores, dim=-1)\n",
    "\n",
    "        # Attention output\n",
    "        att_output = torch.matmul(att_weights, V)  # (batch, num_heads, q_seq_len, head_dim)\n",
    "        \n",
    "        if auto_reg:\n",
    "            combined_heads = att_output.reshape(batch_size, 1, self.hidden_size)\n",
    "        else:\n",
    "            combined_heads = att_output.reshape(batch_size, seq_len, self.hidden_size)\n",
    "\n",
    "        # Apply layer normalization and the final projection\n",
    "        norm_output = self.norm(combined_heads)\n",
    "        projected_output = torch.matmul(norm_output, self.output_proj)\n",
    "\n",
    "        return projected_output\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        self.past_key, self.past_value = None, None\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, hidden_layers=2, act='relu', hidden_size=[32, 32], input_size=32):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(input_size, hidden_size[0])]\n",
    "\n",
    "        for i in range(1, hidden_layers):\n",
    "            if act == 'relu':\n",
    "                layers.append(nn.ReLU())\n",
    "\n",
    "            elif act == 'tanh':\n",
    "                layers.append(nn.Tanh())\n",
    "            layers.append(nn.Linear(hidden_size[i-1], hidden_size[i]))\n",
    "            \n",
    "                \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_seq_len, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "        position = torch.arange(max_seq_len).unsqueeze(1) # (seq_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2) * (-math.log(10000.0) / embed_size))\n",
    "        pe = torch.zeros(1, max_seq_len, embed_size)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term) # (1, max_seq_len, embed_size)\n",
    "        self.register_buffer('pe', pe) # So it goes to CPU/GPU as well\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            X (batch_size, seq_len, embed_size)\n",
    "        \"\"\"\n",
    "        X = X + self.pe[:, :X.shape[1],:]\n",
    "        return self.dropout(X)  \n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, voc_size, embed_size, num_heads, depth, pad_idx, p=0.3):\n",
    "        super().__init__()\n",
    "        self.voc_size = voc_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = depth\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        # Embeddings layer\n",
    "        self.embed = nn.Embedding(voc_size, embed_size)\n",
    "        nn.init.xavier_uniform_(self.embed.weight)\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.pe = PositionalEncoding(embed_size=self.embed_size, max_seq_len=1000).to(device)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "        # Attention Heads\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                MultiAttentionHead(num_heads, embed_size),\n",
    "                nn.LayerNorm(embed_size),\n",
    "                FFN(hidden_layers=2, act=\"relu\",\n",
    "                    hidden_size=[4 * embed_size, embed_size],\n",
    "                    input_size=embed_size),\n",
    "                nn.LayerNorm(embed_size)\n",
    "            ])\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        # Last linear layer\n",
    "        self.linear = nn.Linear(embed_size, voc_size, bias=False)\n",
    "    \n",
    "\n",
    "    def forward(self, X, auto_reg=False):\n",
    "        '''\n",
    "        Inputs:\n",
    "            X (batch_size, seq_length)\n",
    "        '''\n",
    "        _, seq_len = X.shape\n",
    "        padding_mask = X != self.pad_idx\n",
    "\n",
    "        # Positional Encoding\n",
    "\n",
    "        X = self.embed(X) # (batch_size, seq_length, embedding_size)\n",
    "        X = self.pe(X)\n",
    "\n",
    "        if auto_reg:\n",
    "            X = X[:, -1:, :]  # (batch_size, 1, embed_size)\n",
    "\n",
    "        for head, norm1, ffn, norm2 in self.heads:\n",
    "            # Go through the multi heads\n",
    "            X = self.dropout(norm1(head(X, padding_mask, auto_reg) + X))\n",
    "\n",
    "            # Do the same for the FFN\n",
    "            X = self.dropout(norm2(ffn(X) + X))\n",
    "        \n",
    "        output = self.linear(X)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def generate_from_prompt(self, prompt, sample='k', max_size=1000):\n",
    "        self.eval() # We aren't training anymore\n",
    "        self.clear_memo() # Clear hashes\n",
    "        inputs, _ = process_sentence(prompt)\n",
    "        inputs = inputs[1:-1] # Remove <SOS> and <EOS>\n",
    "        curr = len(inputs)\n",
    "        while curr < max_size:\n",
    "            X = torch.tensor(inputs, dtype=torch.long).unsqueeze(0).to(device) # Add batch size 1\n",
    "            with torch.no_grad():\n",
    "                logits = self(X, auto_reg=True)\n",
    "            \n",
    "            # Sample from logits\n",
    "            if sample == 'k':\n",
    "                token = self.top_k_sample(logits)[0, 0].item()\n",
    "                \n",
    "            elif sample == 'greedy':\n",
    "                token = self.greedy_sample(logits)[0, 0].item()\n",
    "            \n",
    "            # See if end of sentence\n",
    "            if idx2word[token] == '<EOS>':\n",
    "                break\n",
    "            \n",
    "            curr += 1\n",
    "            inputs.append(token)\n",
    "        \n",
    "        return decode(inputs)\n",
    "\n",
    "    def clear_memo(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, MultiAttentionHead):\n",
    "                module.clear_cache()\n",
    "\n",
    "\n",
    "    def greedy_sample(self, output):\n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(output, dim=-1)\n",
    "\n",
    "\n",
    "    def top_k_sample(self, output, k=5):\n",
    "        '''\n",
    "        Inputs:\n",
    "            output (batch_size, seq_len, voc_size)\n",
    "        '''\n",
    "        batch_size, seq_len, _ = output.shape \n",
    "        res = torch.zeros((batch_size, seq_len), dtype=torch.long).to(device)\n",
    "        with torch.no_grad():\n",
    "            probabilities = torch.softmax(output, dim=-1)\n",
    "            values, indices = torch.topk(probabilities, k=k, dim=-1)\n",
    "            batch_idx = torch.arange(batch_size)\n",
    "            for pos in range(seq_len): \n",
    "                sample = torch.multinomial(values[:, pos, :], num_samples=1).squeeze(1) # (batch_size)\n",
    "                res[:, pos] = indices[batch_idx, pos, sample]\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "107fc4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: torch.Size([2, 5, 512])\n",
      "torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "# Example dimensions\n",
    "batch_size = 2    # number of sequences\n",
    "seq_len = 5       # tokens per sequence\n",
    "hidden_size = 512   # embedding dimension\n",
    "h = 8 # number of heads\n",
    "head = MultiAttentionHead(h, hidden_size).to(device)\n",
    "\n",
    "# Create random input\n",
    "X = torch.randn(batch_size, seq_len, hidden_size).to(device)\n",
    "att_mask = (torch.randint(0, 2, (batch_size, seq_len)) == 1).to(device)\n",
    "output = head(X, att_mask)\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81777d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(voc_size=len(word2idx), embed_size=512, num_heads=8, depth=12,\\\n",
    "                          pad_idx=word2idx['<PAD>'], p=0.3).to(device)\n",
    "\n",
    "prompt = \"Some years\"\n",
    "ans = transformer.generate_from_prompt(prompt.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2ca8946e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape of k sampling: torch.Size([10, 6])\n",
      "Output shape of greedy sampling: torch.Size([10, 6])\n"
     ]
    }
   ],
   "source": [
    "X_test = torch.tensor(X_pad[:10]).to(device)\n",
    "transformer = Transformer(voc_size=len(word2idx), embed_size=512, num_heads=8, depth=12,\\\n",
    "                          pad_idx=word2idx['<PAD>'], p=0.3).to(device)\n",
    "\n",
    "output = transformer(X_test)\n",
    "\n",
    "prediction_k = transformer.top_k_sample(output)\n",
    "prediction_greedy = transformer.greedy_sample(output)\n",
    "print(f\"Output shape of k sampling: {prediction_k.shape}\")\n",
    "print(f\"Output shape of greedy sampling: {prediction_greedy.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "2353d19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text for k sampling:\n",
      "escaped biased unusual evanescent melodious struggle\n",
      "selecting unthinking oftentimes congeniality outwardly charter\n",
      "mum repentant vagueness highway sagacious tender\n",
      "hopped afflicted worthy cents small-e elevated\n",
      "true—ye palate excitedly wherever undefiled tested\n",
      "morrel aggrieved—this border lying-in it conjure\n",
      "ancestry speaks cultured gouty viewed pomatum\n",
      "gallop sitting organized thrasher threads beaver\n",
      "treacherous wolfish tails mishap undashed death\n",
      "lawless creamy suckling —very stall-fed clouds\n",
      "\n",
      "Text for greedy sampling:\n",
      "speculative precincts content fadest turtles struggle\n",
      "mind—it unthinking waves—the motioned flat-faced kindness\n",
      "torso upright oxygenated highway sagacious now—was\n",
      "me afflicted rise—yes cents small-e water-line\n",
      "sunwards anacharsis prick clifford united hatches—don\n",
      "averse aggrieved—this nate reveries—stand whiteness—though conjure\n",
      "ancestry skylarking comforts escape viewed propped\n",
      "blasted mortar descry formation grapes beaver\n",
      "anchor—as lint comparable codfish enfoldings death\n",
      "insurances to—justice ripening equalled rushing bridegroom\n"
     ]
    }
   ],
   "source": [
    "print(\"Text for k sampling:\")\n",
    "for batch in prediction_k:\n",
    "    print(decode(batch))\n",
    "\n",
    "print(\"\\nText for greedy sampling:\")\n",
    "\n",
    "for batch in prediction_greedy:\n",
    "    print(decode(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "5047b869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, optimizer, scheduler, pad_idx):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "    total_loss = 0.0\n",
    "    for i, (batch_X, batch_y) in enumerate(train_loader):\n",
    "        if i % 30 == 0 or i == len(train_loader) - 1:\n",
    "            print(f\"{(i / len(train_loader)) * 100:.2f}% done\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device) # (batch_size, seq_len)\n",
    "        \n",
    "        outputs = model(batch_X) # (batch_size, seq_len, voc_size)\n",
    "        outputs = outputs.permute(0, 2, 1) # (batch_size, voc_size, seq_len) -> expected size for nn.CrossEntropy\n",
    "\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() \n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    return total_loss / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "816f7984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_transformer_training(model, total_steps, warmup_steps):\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=5e-4,             \n",
    "        weight_decay=0.01   \n",
    "    )\n",
    "\n",
    "    # Learning rate schedule with warmup + linear decay to zero\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lr_lambda=lambda step: min((step + 1) / warmup_steps, \n",
    "                                   max(0.0, (total_steps - step) / max(1, total_steps - warmup_steps)))\n",
    "    )\n",
    "\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a4e1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "0.00% done\n",
      "18.63% done\n",
      "37.27% done\n",
      "55.90% done\n",
      "74.53% done\n",
      "93.17% done\n",
      "99.38% done\n",
      "Loss: 0.4878241716420595\n",
      "Found better model at epoch 1. Saving...\n",
      "Epoch 2\n",
      "0.00% done\n",
      "18.63% done\n",
      "37.27% done\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "warmup_epochs = 10\n",
    "pad_idx = word2idx['<PAD>']\n",
    "model = Transformer(voc_size=len(word2idx), embed_size=512, num_heads=8, depth=12,\\\n",
    "                    pad_idx=word2idx['<PAD>'], p=0.3).to(device)\n",
    "opt, scheduler = setup_transformer_training(model, num_epochs, warmup_epochs)\n",
    "\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    loss = train_one_epoch(model, dataloader, opt, scheduler, pad_idx)\n",
    "    print(f\"Loss: {loss}\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        print(f\"Found better model at epoch {epoch}. Saving...\")\n",
    "        torch.save(model.state_dict(), \"models/best_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
